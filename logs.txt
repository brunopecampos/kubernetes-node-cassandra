* 
* ==> Audit <==
* |--------------|-------------------------------------------------------------------------------------|--------------------|----------|---------|---------------------|---------------------|
|   Command    |                                        Args                                         |      Profile       |   User   | Version |     Start Time      |      End Time       |
|--------------|-------------------------------------------------------------------------------------|--------------------|----------|---------|---------------------|---------------------|
| update-check |                                                                                     | minikube           | brunopec | v1.27.1 | 03 Jan 23 08:25 -03 | 03 Jan 23 08:25 -03 |
| start        |                                                                                     | minikube           | brunopec | v1.27.1 | 03 Jan 23 08:25 -03 | 03 Jan 23 08:26 -03 |
| stop         |                                                                                     | minikube           | brunopec | v1.27.1 | 03 Jan 23 08:39 -03 | 03 Jan 23 08:39 -03 |
| start        | -p k8s-node-cassandra -n 3                                                          | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:41 -03 | 03 Jan 23 08:43 -03 |
| profile      |                                                                                     | minikube           | brunopec | v1.27.1 | 03 Jan 23 08:46 -03 |                     |
| profile      | list                                                                                | minikube           | brunopec | v1.27.1 | 03 Jan 23 08:46 -03 | 03 Jan 23 08:46 -03 |
| profile      | k8s-node-cassandra                                                                  | minikube           | brunopec | v1.27.1 | 03 Jan 23 08:46 -03 | 03 Jan 23 08:46 -03 |
| profile      |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:47 -03 |                     |
| profile      | list                                                                                | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:47 -03 | 03 Jan 23 08:47 -03 |
| start        | -p k8s-node-cassandra -n 3                                                          | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:47 -03 | 03 Jan 23 08:48 -03 |
| profile      | k8s-node-cassandra                                                                  | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:48 -03 | 03 Jan 23 08:48 -03 |
| start        | -p k8s-node-cassandra -n 3                                                          | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:49 -03 | 03 Jan 23 08:50 -03 |
| profile      | k8s-node-cassandra                                                                  | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:50 -03 | 03 Jan 23 08:50 -03 |
| start        | -p k8s-node-cassandra -n 3                                                          | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:54 -03 | 03 Jan 23 08:54 -03 |
| profile      | k8s-node-cassandra                                                                  | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:54 -03 | 03 Jan 23 08:54 -03 |
| start        | -p k8s-node-cassandra -n 3                                                          | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:56 -03 | 03 Jan 23 08:57 -03 |
| profile      | k8s-node-cassandra                                                                  | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:57 -03 | 03 Jan 23 08:57 -03 |
| start        | -p k8s-node-cassandra -n 3                                                          | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:58 -03 | 03 Jan 23 08:59 -03 |
| profile      | k8s-node-cassandra                                                                  | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:59 -03 | 03 Jan 23 08:59 -03 |
| addons       | enable ingress                                                                      | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 08:59 -03 | 03 Jan 23 09:04 -03 |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 09:06 -03 |                     |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 09:07 -03 | 03 Jan 23 09:07 -03 |
| cp           | enc.yaml                                                                            | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:34 -03 |                     |
|              | /etc/kubernetes/enc/enc.yaml                                                        |                    |          |         |                     |                     |
| profile      | list                                                                                | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:35 -03 | 03 Jan 23 10:35 -03 |
| cp           |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:35 -03 |                     |
| cp           | /home/brunopec/Documents/opus/kubernetes/kubernetes-node-cassandra/secrets/enc.yaml | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:36 -03 |                     |
|              | /etc/kubernetes/enc/enc.yaml                                                        |                    |          |         |                     |                     |
| node         |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:37 -03 |                     |
| node         | list                                                                                | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:37 -03 |                     |
| cp           | /home/brunopec/Documents/opus/kubernetes/kubernetes-node-cassandra/secrets/enc.yaml | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:39 -03 |                     |
|              | /etc/kubernetes/enc/enc.yaml                                                        |                    |          |         |                     |                     |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:39 -03 | 03 Jan 23 10:40 -03 |
| cp           | /home/brunopec/Documents/opus/kubernetes/kubernetes-node-cassandra/secrets/enc.yaml | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:40 -03 | 03 Jan 23 10:40 -03 |
|              | /etc/kubernetes/enc/enc.yaml                                                        |                    |          |         |                     |                     |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:41 -03 |                     |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:45 -03 |                     |
| start        |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:49 -03 | 03 Jan 23 10:50 -03 |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:53 -03 | 03 Jan 23 10:56 -03 |
| cp           | /home/brunopec/Downloads/etcd-v3.4.23-linux-amd64/etcdctl                           | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:57 -03 | 03 Jan 23 10:57 -03 |
|              | /tpm                                                                                |                    |          |         |                     |                     |
| cp           | /home/brunopec/Downloads/etcd-v3.4.23-linux-amd64/etcdctl                           | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:57 -03 |                     |
|              | /tmp                                                                                |                    |          |         |                     |                     |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:57 -03 |                     |
| cp           | /home/brunopec/Downloads/etcd-v3.4.23-linux-amd64/etcdctl                           | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:58 -03 |                     |
|              | /tmp                                                                                |                    |          |         |                     |                     |
| cp           | /home/brunopec/Downloads/etcd-v3.4.23-linux-amd64/etcdctl                           | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:58 -03 | 03 Jan 23 10:58 -03 |
|              | /tmp/etcdctl                                                                        |                    |          |         |                     |                     |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 10:58 -03 |                     |
| ssh          |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 11:13 -03 |                     |
| cp           | /usr/bin/hexdump /tmp                                                               | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 11:14 -03 |                     |
| cp           | /usr/bin/hexdump /tmp/hexdump                                                       | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 11:14 -03 | 03 Jan 23 11:14 -03 |
| addons       |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 13:41 -03 | 03 Jan 23 13:41 -03 |
| addons       | list                                                                                | k8s-node-cassandra | brunopec | v1.27.1 | 03 Jan 23 13:41 -03 | 03 Jan 23 13:41 -03 |
| profile      |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 04 Jan 23 10:36 -03 |                     |
| profile      | list                                                                                | k8s-node-cassandra | brunopec | v1.27.1 | 04 Jan 23 10:36 -03 | 04 Jan 23 10:36 -03 |
| profile      | minikube                                                                            | k8s-node-cassandra | brunopec | v1.27.1 | 04 Jan 23 10:36 -03 | 04 Jan 23 10:36 -03 |
| start        |                                                                                     | minikube           | brunopec | v1.27.1 | 04 Jan 23 10:40 -03 | 04 Jan 23 10:41 -03 |
| addons       |                                                                                     | minikube           | brunopec | v1.27.1 | 04 Jan 23 10:47 -03 | 04 Jan 23 10:47 -03 |
| addons       | list                                                                                | minikube           | brunopec | v1.27.1 | 04 Jan 23 10:47 -03 | 04 Jan 23 10:47 -03 |
| profile      | list                                                                                | minikube           | brunopec | v1.27.1 | 04 Jan 23 10:48 -03 | 04 Jan 23 10:48 -03 |
| update-check |                                                                                     | minikube           | brunopec | v1.27.1 | 04 Jan 23 13:22 -03 | 04 Jan 23 13:22 -03 |
| stop         |                                                                                     | minikube           | brunopec | v1.27.1 | 04 Jan 23 13:22 -03 | 04 Jan 23 13:23 -03 |
| profile      | list                                                                                | minikube           | brunopec | v1.27.1 | 04 Jan 23 13:25 -03 | 04 Jan 23 13:25 -03 |
| profile      | k8s-node-cassandra                                                                  | minikube           | brunopec | v1.27.1 | 04 Jan 23 13:25 -03 | 04 Jan 23 13:25 -03 |
| start        |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 04 Jan 23 13:25 -03 |                     |
| profile      | list                                                                                | k8s-node-cassandra | brunopec | v1.27.1 | 04 Jan 23 13:32 -03 | 04 Jan 23 13:32 -03 |
| start        |                                                                                     | k8s-node-cassandra | brunopec | v1.27.1 | 04 Jan 23 13:35 -03 |                     |
|--------------|-------------------------------------------------------------------------------------|--------------------|----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/01/04 13:35:58
Running on machine: archlinux
Binary: Built with gc go1.19.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0104 13:35:58.852568  608550 out.go:296] Setting OutFile to fd 1 ...
I0104 13:35:58.852658  608550 out.go:348] isatty.IsTerminal(1) = true
I0104 13:35:58.852660  608550 out.go:309] Setting ErrFile to fd 2...
I0104 13:35:58.852663  608550 out.go:348] isatty.IsTerminal(2) = true
I0104 13:35:58.852730  608550 root.go:333] Updating PATH: /home/brunopec/.minikube/bin
I0104 13:35:58.853026  608550 out.go:303] Setting JSON to false
I0104 13:35:58.854233  608550 start.go:116] hostinfo: {"hostname":"archlinux","uptime":19725,"bootTime":1672830434,"procs":403,"os":"linux","platform":"arch","platformFamily":"arch","platformVersion":"\"rolling\"","kernelVersion":"6.0.8-arch1-1","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"7084d3d4-dc9b-414e-b724-1964e94b4c4f"}
I0104 13:35:58.854269  608550 start.go:126] virtualization: kvm host
I0104 13:35:58.855880  608550 out.go:177] 😄  [k8s-node-cassandra] minikube v1.27.1 on Arch "rolling"
I0104 13:35:58.856863  608550 notify.go:220] Checking for updates...
I0104 13:35:58.857066  608550 config.go:180] Loaded profile config "k8s-node-cassandra": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I0104 13:35:58.857097  608550 driver.go:362] Setting default libvirt URI to qemu:///system
I0104 13:35:58.878127  608550 docker.go:137] docker version: linux-20.10.21
I0104 13:35:58.878205  608550 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0104 13:35:58.910693  608550 info.go:265] docker info: {ID:FZBI:O2HM:Y5OO:SARF:MOXC:YSFD:ABGD:ED2K:JNE6:TIAG:VX3Z:ORJR Containers:14 ContainersRunning:2 ContainersPaused:0 ContainersStopped:12 Images:87 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:43 OomKillDisable:false NGoroutines:50 SystemTime:2023-01-04 13:35:58.891087063 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.0.8-arch1-1 OperatingSystem:Arch Linux OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16567197696 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:archlinux Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f.m Expected:1c90a442489720eec95342e1789ee8a5e1b9536f.m} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:r3411.1658828475.ce1829a46]] Warnings:<nil>}}
I0104 13:35:58.910751  608550 docker.go:254] overlay module found
I0104 13:35:58.912141  608550 out.go:177] ✨  Using the docker driver based on existing profile
I0104 13:35:58.913036  608550 start.go:282] selected driver: docker
I0104 13:35:58.913041  608550 start.go:808] validating driver "docker" against &{Name:k8s-node-cassandra KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.2 ClusterName:k8s-node-cassandra Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/brunopec:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0104 13:35:58.913119  608550 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0104 13:35:58.913174  608550 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0104 13:35:58.945140  608550 info.go:265] docker info: {ID:FZBI:O2HM:Y5OO:SARF:MOXC:YSFD:ABGD:ED2K:JNE6:TIAG:VX3Z:ORJR Containers:14 ContainersRunning:2 ContainersPaused:0 ContainersStopped:12 Images:87 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff false] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:43 OomKillDisable:false NGoroutines:50 SystemTime:2023-01-04 13:35:58.926539559 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.0.8-arch1-1 OperatingSystem:Arch Linux OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:16567197696 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:archlinux Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f.m Expected:1c90a442489720eec95342e1789ee8a5e1b9536f.m} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:r3411.1658828475.ce1829a46]] Warnings:<nil>}}
I0104 13:35:58.945547  608550 cni.go:95] Creating CNI manager for ""
I0104 13:35:58.945551  608550 cni.go:156] 3 nodes found, recommending kindnet
I0104 13:35:58.945764  608550 start_flags.go:317] config:
{Name:k8s-node-cassandra KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.2 ClusterName:k8s-node-cassandra Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/brunopec:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0104 13:35:58.947791  608550 out.go:177] 👍  Starting control plane node k8s-node-cassandra in cluster k8s-node-cassandra
I0104 13:35:58.948681  608550 cache.go:120] Beginning downloading kic base image for docker with docker
I0104 13:35:58.949593  608550 out.go:177] 🚜  Pulling base image ...
I0104 13:35:58.951240  608550 preload.go:132] Checking if preload exists for k8s version v1.25.2 and runtime docker
I0104 13:35:58.951258  608550 preload.go:148] Found local preload: /home/brunopec/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.2-docker-overlay2-amd64.tar.lz4
I0104 13:35:58.951264  608550 cache.go:57] Caching tarball of preloaded images
I0104 13:35:58.951315  608550 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b in local docker daemon
I0104 13:35:58.951412  608550 preload.go:174] Found /home/brunopec/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.2-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0104 13:35:58.951425  608550 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.2 on docker
I0104 13:35:58.951529  608550 profile.go:148] Saving config to /home/brunopec/.minikube/profiles/k8s-node-cassandra/config.json ...
I0104 13:35:58.980723  608550 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b in local docker daemon, skipping pull
I0104 13:35:58.980735  608550 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b exists in daemon, skipping load
I0104 13:35:58.980743  608550 cache.go:208] Successfully downloaded all kic artifacts
I0104 13:35:58.980765  608550 start.go:364] acquiring machines lock for k8s-node-cassandra: {Name:mkfc76aa8c0515d82a714afb294d4c6cff9a6bb7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 13:35:58.980843  608550 start.go:368] acquired machines lock for "k8s-node-cassandra" in 56.707µs
I0104 13:35:58.980859  608550 start.go:96] Skipping create...Using existing machine configuration
I0104 13:35:58.980862  608550 fix.go:55] fixHost starting: 
I0104 13:35:58.981019  608550 cli_runner.go:164] Run: docker container inspect k8s-node-cassandra --format={{.State.Status}}
I0104 13:35:58.995593  608550 fix.go:103] recreateIfNeeded on k8s-node-cassandra: state=Running err=<nil>
W0104 13:35:58.995605  608550 fix.go:129] unexpected machine state, will restart: <nil>
I0104 13:35:58.996481  608550 out.go:177] 🏃  Updating the running docker "k8s-node-cassandra" container ...
I0104 13:35:58.997395  608550 machine.go:88] provisioning docker machine ...
I0104 13:35:58.997408  608550 ubuntu.go:169] provisioning hostname "k8s-node-cassandra"
I0104 13:35:58.997442  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:35:59.017724  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:35:59.017854  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I0104 13:35:59.017861  608550 main.go:134] libmachine: About to run SSH command:
sudo hostname k8s-node-cassandra && echo "k8s-node-cassandra" | sudo tee /etc/hostname
I0104 13:35:59.138105  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: k8s-node-cassandra

I0104 13:35:59.138173  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:35:59.156264  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:35:59.156405  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I0104 13:35:59.156421  608550 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sk8s-node-cassandra' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 k8s-node-cassandra/g' /etc/hosts;
			else 
				echo '127.0.1.1 k8s-node-cassandra' | sudo tee -a /etc/hosts; 
			fi
		fi
I0104 13:35:59.267931  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0104 13:35:59.267970  608550 ubuntu.go:175] set auth options {CertDir:/home/brunopec/.minikube CaCertPath:/home/brunopec/.minikube/certs/ca.pem CaPrivateKeyPath:/home/brunopec/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/brunopec/.minikube/machines/server.pem ServerKeyPath:/home/brunopec/.minikube/machines/server-key.pem ClientKeyPath:/home/brunopec/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/brunopec/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/brunopec/.minikube}
I0104 13:35:59.268004  608550 ubuntu.go:177] setting up certificates
I0104 13:35:59.268016  608550 provision.go:83] configureAuth start
I0104 13:35:59.268110  608550 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" k8s-node-cassandra
I0104 13:35:59.281023  608550 provision.go:138] copyHostCerts
I0104 13:35:59.281057  608550 exec_runner.go:144] found /home/brunopec/.minikube/ca.pem, removing ...
I0104 13:35:59.281062  608550 exec_runner.go:207] rm: /home/brunopec/.minikube/ca.pem
I0104 13:35:59.281114  608550 exec_runner.go:151] cp: /home/brunopec/.minikube/certs/ca.pem --> /home/brunopec/.minikube/ca.pem (1082 bytes)
I0104 13:35:59.281163  608550 exec_runner.go:144] found /home/brunopec/.minikube/cert.pem, removing ...
I0104 13:35:59.281166  608550 exec_runner.go:207] rm: /home/brunopec/.minikube/cert.pem
I0104 13:35:59.281182  608550 exec_runner.go:151] cp: /home/brunopec/.minikube/certs/cert.pem --> /home/brunopec/.minikube/cert.pem (1127 bytes)
I0104 13:35:59.281204  608550 exec_runner.go:144] found /home/brunopec/.minikube/key.pem, removing ...
I0104 13:35:59.281207  608550 exec_runner.go:207] rm: /home/brunopec/.minikube/key.pem
I0104 13:35:59.281224  608550 exec_runner.go:151] cp: /home/brunopec/.minikube/certs/key.pem --> /home/brunopec/.minikube/key.pem (1679 bytes)
I0104 13:35:59.281246  608550 provision.go:112] generating server cert: /home/brunopec/.minikube/machines/server.pem ca-key=/home/brunopec/.minikube/certs/ca.pem private-key=/home/brunopec/.minikube/certs/ca-key.pem org=brunopec.k8s-node-cassandra san=[192.168.58.2 127.0.0.1 localhost 127.0.0.1 minikube k8s-node-cassandra]
I0104 13:35:59.375615  608550 provision.go:172] copyRemoteCerts
I0104 13:35:59.375656  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0104 13:35:59.375684  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:35:59.389926  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:35:59.471436  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0104 13:35:59.483127  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/machines/server.pem --> /etc/docker/server.pem (1233 bytes)
I0104 13:35:59.496162  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0104 13:35:59.507937  608550 provision.go:86] duration metric: configureAuth took 239.912378ms
I0104 13:35:59.507949  608550 ubuntu.go:193] setting minikube options for container-runtime
I0104 13:35:59.508071  608550 config.go:180] Loaded profile config "k8s-node-cassandra": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I0104 13:35:59.508109  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:35:59.522903  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:35:59.522996  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I0104 13:35:59.523002  608550 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0104 13:35:59.634694  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0104 13:35:59.634717  608550 ubuntu.go:71] root file system type: overlay
I0104 13:35:59.635032  608550 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0104 13:35:59.635125  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:35:59.649842  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:35:59.649933  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I0104 13:35:59.649976  608550 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0104 13:35:59.771048  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0104 13:35:59.771097  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:35:59.784942  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:35:59.785106  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I0104 13:35:59.785121  608550 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0104 13:35:59.915937  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0104 13:35:59.915948  608550 machine.go:91] provisioned docker machine in 918.548632ms
I0104 13:35:59.915955  608550 start.go:300] post-start starting for "k8s-node-cassandra" (driver="docker")
I0104 13:35:59.915959  608550 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0104 13:35:59.916003  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0104 13:35:59.916031  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:35:59.948393  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:00.045577  608550 ssh_runner.go:195] Run: cat /etc/os-release
I0104 13:36:00.047308  608550 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0104 13:36:00.047324  608550 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0104 13:36:00.047333  608550 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0104 13:36:00.047338  608550 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0104 13:36:00.047347  608550 filesync.go:126] Scanning /home/brunopec/.minikube/addons for local assets ...
I0104 13:36:00.047380  608550 filesync.go:126] Scanning /home/brunopec/.minikube/files for local assets ...
I0104 13:36:00.047394  608550 start.go:303] post-start completed in 131.434899ms
I0104 13:36:00.047444  608550 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0104 13:36:00.047481  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:00.064901  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:00.141491  608550 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0104 13:36:00.143927  608550 fix.go:57] fixHost completed within 1.163060939s
I0104 13:36:00.143937  608550 start.go:83] releasing machines lock for "k8s-node-cassandra", held for 1.163088618s
I0104 13:36:00.143992  608550 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" k8s-node-cassandra
I0104 13:36:00.162226  608550 ssh_runner.go:195] Run: systemctl --version
I0104 13:36:00.162268  608550 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0104 13:36:00.162271  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:00.162306  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:00.181491  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:00.181925  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:00.969908  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0104 13:36:00.974193  608550 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (234 bytes)
I0104 13:36:00.981519  608550 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 13:36:01.094286  608550 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0104 13:36:01.166776  608550 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0104 13:36:01.173109  608550 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0104 13:36:01.173148  608550 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0104 13:36:01.178804  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0104 13:36:01.186463  608550 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0104 13:36:01.290027  608550 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0104 13:36:01.415542  608550 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 13:36:01.515390  608550 ssh_runner.go:195] Run: sudo systemctl restart docker
I0104 13:36:12.386619  608550 ssh_runner.go:235] Completed: sudo systemctl restart docker: (10.871213045s)
I0104 13:36:12.386665  608550 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0104 13:36:12.515420  608550 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 13:36:12.691674  608550 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0104 13:36:12.736206  608550 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0104 13:36:12.736272  608550 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0104 13:36:12.742129  608550 start.go:472] Will wait 60s for crictl version
I0104 13:36:12.742200  608550 ssh_runner.go:195] Run: sudo crictl version
I0104 13:36:12.854003  608550 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.18
RuntimeApiVersion:  1.41.0
I0104 13:36:12.854068  608550 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 13:36:12.907287  608550 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 13:36:12.948624  608550 out.go:204] 🐳  Preparing Kubernetes v1.25.2 on Docker 20.10.18 ...
I0104 13:36:12.948801  608550 cli_runner.go:164] Run: docker network inspect k8s-node-cassandra --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0104 13:36:12.977197  608550 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0104 13:36:12.983263  608550 preload.go:132] Checking if preload exists for k8s version v1.25.2 and runtime docker
I0104 13:36:12.983328  608550 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0104 13:36:13.011034  608550 docker.go:611] Got preloaded images: -- stdout --
bitnami/cassandra:4.0.7
bitnami/nginx:1.23.3-debian-11-r2
bitnami/git:2.39.0-debian-11-r2
kindest/kindnetd:v20221004-44d545d1
registry.k8s.io/kube-apiserver:v1.25.2
registry.k8s.io/kube-scheduler:v1.25.2
registry.k8s.io/kube-controller-manager:v1.25.2
registry.k8s.io/kube-proxy:v1.25.2
registry.k8s.io/sig-storage/nfsplugin:v4.1.0
registry.k8s.io/pause:3.8
registry.k8s.io/sig-storage/csi-provisioner:v3.2.0
registry.k8s.io/etcd:3.5.4-0
k8s.gcr.io/ingress-nginx/controller:<none>
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/sig-storage/livenessprobe:v2.7.0
registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.5.1
k8s.gcr.io/ingress-nginx/kube-webhook-certgen:<none>
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0104 13:36:13.011048  608550 docker.go:542] Images already preloaded, skipping extraction
I0104 13:36:13.011091  608550 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0104 13:36:13.047795  608550 docker.go:611] Got preloaded images: -- stdout --
bitnami/cassandra:4.0.7
bitnami/nginx:1.23.3-debian-11-r2
bitnami/git:2.39.0-debian-11-r2
kindest/kindnetd:v20221004-44d545d1
registry.k8s.io/kube-apiserver:v1.25.2
registry.k8s.io/kube-controller-manager:v1.25.2
registry.k8s.io/kube-scheduler:v1.25.2
registry.k8s.io/kube-proxy:v1.25.2
registry.k8s.io/sig-storage/nfsplugin:v4.1.0
registry.k8s.io/pause:3.8
registry.k8s.io/sig-storage/csi-provisioner:v3.2.0
registry.k8s.io/etcd:3.5.4-0
k8s.gcr.io/ingress-nginx/controller:<none>
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/sig-storage/livenessprobe:v2.7.0
registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.5.1
k8s.gcr.io/ingress-nginx/kube-webhook-certgen:<none>
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0104 13:36:13.047808  608550 cache_images.go:84] Images are preloaded, skipping loading
I0104 13:36:13.047873  608550 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0104 13:36:13.840748  608550 cni.go:95] Creating CNI manager for ""
I0104 13:36:13.840760  608550 cni.go:156] 3 nodes found, recommending kindnet
I0104 13:36:13.840772  608550 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0104 13:36:13.840784  608550 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.25.2 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:k8s-node-cassandra NodeName:k8s-node-cassandra DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I0104 13:36:13.840922  608550 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "k8s-node-cassandra"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.2
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0104 13:36:13.841006  608550 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.2/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=k8s-node-cassandra --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.2 ClusterName:k8s-node-cassandra Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0104 13:36:13.841091  608550 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.2
I0104 13:36:13.850292  608550 binaries.go:44] Found k8s binaries, skipping transfer
I0104 13:36:13.850376  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0104 13:36:13.873486  608550 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (480 bytes)
I0104 13:36:13.902735  608550 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0104 13:36:13.925852  608550 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2040 bytes)
I0104 13:36:13.945582  608550 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0104 13:36:13.951215  608550 certs.go:54] Setting up /home/brunopec/.minikube/profiles/k8s-node-cassandra for IP: 192.168.58.2
I0104 13:36:13.951308  608550 certs.go:182] skipping minikubeCA CA generation: /home/brunopec/.minikube/ca.key
I0104 13:36:13.951341  608550 certs.go:182] skipping proxyClientCA CA generation: /home/brunopec/.minikube/proxy-client-ca.key
I0104 13:36:13.951403  608550 certs.go:298] skipping minikube-user signed cert generation: /home/brunopec/.minikube/profiles/k8s-node-cassandra/client.key
I0104 13:36:13.951452  608550 certs.go:298] skipping minikube signed cert generation: /home/brunopec/.minikube/profiles/k8s-node-cassandra/apiserver.key.cee25041
I0104 13:36:13.951484  608550 certs.go:298] skipping aggregator signed cert generation: /home/brunopec/.minikube/profiles/k8s-node-cassandra/proxy-client.key
I0104 13:36:13.951556  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/ca-key.pem (1675 bytes)
I0104 13:36:13.951577  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/ca.pem (1082 bytes)
I0104 13:36:13.951594  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/cert.pem (1127 bytes)
I0104 13:36:13.951610  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/key.pem (1679 bytes)
I0104 13:36:13.952247  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/profiles/k8s-node-cassandra/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0104 13:36:13.992954  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/profiles/k8s-node-cassandra/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0104 13:36:14.027714  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/profiles/k8s-node-cassandra/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0104 13:36:14.056362  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/profiles/k8s-node-cassandra/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0104 13:36:14.096169  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0104 13:36:14.130616  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0104 13:36:14.163179  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0104 13:36:14.202675  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0104 13:36:14.228127  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0104 13:36:14.259241  608550 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0104 13:36:14.281419  608550 ssh_runner.go:195] Run: openssl version
I0104 13:36:14.288279  608550 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0104 13:36:14.301587  608550 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0104 13:36:14.305511  608550 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Nov  1 21:30 /usr/share/ca-certificates/minikubeCA.pem
I0104 13:36:14.305590  608550 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0104 13:36:14.310730  608550 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0104 13:36:14.319792  608550 kubeadm.go:396] StartCluster: {Name:k8s-node-cassandra KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.2 ClusterName:k8s-node-cassandra Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/brunopec:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0104 13:36:14.319967  608550 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0104 13:36:14.347616  608550 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0104 13:36:14.353430  608550 kubeadm.go:411] found existing configuration files, will attempt cluster restart
I0104 13:36:14.353442  608550 kubeadm.go:627] restartCluster start
I0104 13:36:14.353497  608550 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0104 13:36:14.359361  608550 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0104 13:36:14.360041  608550 kubeconfig.go:92] found "k8s-node-cassandra" server: "https://192.168.58.2:8443"
I0104 13:36:14.363003  608550 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0104 13:36:14.369974  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:14.370032  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:14.378336  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:14.579440  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:14.579560  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:14.586686  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:14.779286  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:14.779409  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:14.785081  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:14.978997  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:14.979147  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:14.987214  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:15.179458  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:15.179556  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:15.194410  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:15.379055  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:15.379148  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:15.385195  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:15.579151  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:15.579301  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:15.585788  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:15.778676  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:15.778749  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:15.790810  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:15.978785  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:15.978923  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:15.993853  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:16.178792  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:16.178858  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:16.186300  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:16.378981  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:16.379038  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:16.385214  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:16.579226  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:16.579314  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:16.586654  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:16.778423  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:16.778536  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:16.793476  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:16.978474  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:16.978600  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:16.993508  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:17.179278  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:17.179337  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:17.186242  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:17.379113  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:17.379212  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:17.396460  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:17.396470  608550 api_server.go:165] Checking apiserver status ...
I0104 13:36:17.396509  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0104 13:36:17.404722  608550 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0104 13:36:17.404744  608550 kubeadm.go:602] needs reconfigure: apiserver error: timed out waiting for the condition
I0104 13:36:17.404749  608550 kubeadm.go:1114] stopping kube-system containers ...
I0104 13:36:17.404813  608550 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0104 13:36:17.435837  608550 docker.go:443] Stopping containers: [f8d8ab2a9624 bc617017f765 d80f8b672c04 eb1fc7afba15 fa026835d8b9 db01d9e60bd6 80219953857a 667442ea6481 ecbb48f8e31b ba955d8a3949 c4d6364a88ed 5c4c76e6b7bf b36cf481d59a 7f2aecf5d191 7b19508f34bf c7ef95ae4d69 d0c5498e3416 cb42e07a11cd d1f9d2e9f43d c541a7187230 42ff7d6fcc60 5121453a7c25 fa0ffdf9716a c9ff28d65830 fe434e222853 5ecda47cfab3 77b12dd4c32d f9a346d33049 0deb776718fd 610406ace289 695a085509a3 d6c0471065b1 43475311d562 3496498a8a4b b8943c41c1a4 f262489e0062 3ddf59bd1b37 06d0c58db3a9 79f92768a1b0 a6b89aad499a f853e66d4c0b 2e919da0ed0e a524d9356aab 1854751d88cf f64a7f87557c 0b01c918af05 0fd2685a36ed 317d2deb59e1 6503273935c9 d1960402f197 938839d23733 46791c974b1b ba24e6f3ca6a e25f181752d9 d526a7e7f1f5 303ce8887ed8 857b17c6697e 12a4a7e0afa1]
I0104 13:36:17.435902  608550 ssh_runner.go:195] Run: docker stop f8d8ab2a9624 bc617017f765 d80f8b672c04 eb1fc7afba15 fa026835d8b9 db01d9e60bd6 80219953857a 667442ea6481 ecbb48f8e31b ba955d8a3949 c4d6364a88ed 5c4c76e6b7bf b36cf481d59a 7f2aecf5d191 7b19508f34bf c7ef95ae4d69 d0c5498e3416 cb42e07a11cd d1f9d2e9f43d c541a7187230 42ff7d6fcc60 5121453a7c25 fa0ffdf9716a c9ff28d65830 fe434e222853 5ecda47cfab3 77b12dd4c32d f9a346d33049 0deb776718fd 610406ace289 695a085509a3 d6c0471065b1 43475311d562 3496498a8a4b b8943c41c1a4 f262489e0062 3ddf59bd1b37 06d0c58db3a9 79f92768a1b0 a6b89aad499a f853e66d4c0b 2e919da0ed0e a524d9356aab 1854751d88cf f64a7f87557c 0b01c918af05 0fd2685a36ed 317d2deb59e1 6503273935c9 d1960402f197 938839d23733 46791c974b1b ba24e6f3ca6a e25f181752d9 d526a7e7f1f5 303ce8887ed8 857b17c6697e 12a4a7e0afa1
I0104 13:36:17.808538  608550 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0104 13:36:17.881278  608550 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0104 13:36:17.887446  608550 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Jan  3 11:41 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Jan  4 16:25 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 2011 Jan  3 11:42 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jan  4 16:25 /etc/kubernetes/scheduler.conf

I0104 13:36:17.887508  608550 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0104 13:36:17.895367  608550 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0104 13:36:17.900043  608550 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0104 13:36:17.907026  608550 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0104 13:36:17.907074  608550 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0104 13:36:17.913369  608550 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0104 13:36:17.918240  608550 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0104 13:36:17.918295  608550 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0104 13:36:17.924047  608550 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0104 13:36:17.929431  608550 kubeadm.go:704] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0104 13:36:17.929441  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0104 13:36:17.957129  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0104 13:36:18.408959  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0104 13:36:18.559583  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0104 13:36:18.598301  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0104 13:36:18.643075  608550 api_server.go:51] waiting for apiserver process to appear ...
I0104 13:36:18.643190  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 13:36:19.157111  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 13:36:19.656819  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 13:36:20.156439  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 13:36:20.656074  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 13:36:20.662418  608550 api_server.go:71] duration metric: took 2.019364027s to wait for apiserver process to appear ...
I0104 13:36:20.662433  608550 api_server.go:87] waiting for apiserver healthz status ...
I0104 13:36:20.662441  608550 api_server.go:252] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0104 13:36:22.514843  608550 api_server.go:278] https://192.168.58.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0104 13:36:22.514865  608550 api_server.go:102] status: https://192.168.58.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0104 13:36:23.015377  608550 api_server.go:252] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0104 13:36:23.020742  608550 api_server.go:278] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0104 13:36:23.020758  608550 api_server.go:102] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0104 13:36:23.516163  608550 api_server.go:252] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0104 13:36:23.520654  608550 api_server.go:278] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0104 13:36:23.520666  608550 api_server.go:102] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0104 13:36:24.015169  608550 api_server.go:252] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0104 13:36:24.023645  608550 api_server.go:278] https://192.168.58.2:8443/healthz returned 200:
ok
I0104 13:36:24.035862  608550 api_server.go:140] control plane version: v1.25.2
I0104 13:36:24.035884  608550 api_server.go:130] duration metric: took 3.373446247s to wait for apiserver health ...
I0104 13:36:24.035893  608550 cni.go:95] Creating CNI manager for ""
I0104 13:36:24.035900  608550 cni.go:156] 3 nodes found, recommending kindnet
I0104 13:36:24.037444  608550 out.go:177] 🔗  Configuring CNI (Container Networking Interface) ...
I0104 13:36:24.038991  608550 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0104 13:36:24.045436  608550 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.25.2/kubectl ...
I0104 13:36:24.045489  608550 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I0104 13:36:24.074132  608550 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.25.2/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0104 13:36:24.619954  608550 system_pods.go:43] waiting for kube-system pods to appear ...
I0104 13:36:24.627059  608550 system_pods.go:59] 16 kube-system pods found
I0104 13:36:24.627069  608550 system_pods.go:61] "coredns-565d847f94-rqqr2" [2c299796-87a7-48fb-b68d-20f55c4fa07d] Running
I0104 13:36:24.627072  608550 system_pods.go:61] "csi-nfs-controller-679b6d5754-lzrjk" [03ff925d-e48d-46bf-8fa0-5299f446b61c] Running
I0104 13:36:24.627074  608550 system_pods.go:61] "csi-nfs-node-2n65m" [ff7d8c2e-c2fa-465a-894c-4922ac01b8d1] Running
I0104 13:36:24.627076  608550 system_pods.go:61] "csi-nfs-node-fzbkq" [54cc4b6a-2718-4c46-b33d-fda54b106d70] Running
I0104 13:36:24.627079  608550 system_pods.go:61] "csi-nfs-node-qh9kd" [93ad38d6-6476-4a84-bbec-61594c1ac28d] Running
I0104 13:36:24.627081  608550 system_pods.go:61] "etcd-k8s-node-cassandra" [6bc29607-e51f-47f2-b5ff-c6488844dc17] Running
I0104 13:36:24.627085  608550 system_pods.go:61] "kindnet-6sckd" [3561a674-4aba-431c-9a21-3d607358be50] Running / Ready:ContainersNotReady (containers with unready status: [kindnet-cni]) / ContainersReady:ContainersNotReady (containers with unready status: [kindnet-cni])
I0104 13:36:24.627088  608550 system_pods.go:61] "kindnet-hkj5s" [650f220e-ba2e-4bf4-86db-f1517dee4de5] Running
I0104 13:36:24.627090  608550 system_pods.go:61] "kindnet-zxfwt" [4b4f869e-2080-47cc-94f0-6a4102f75be7] Running
I0104 13:36:24.627094  608550 system_pods.go:61] "kube-apiserver-k8s-node-cassandra" [c8776e9a-597d-47c1-9495-6e4b6dedc1c9] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0104 13:36:24.627097  608550 system_pods.go:61] "kube-controller-manager-k8s-node-cassandra" [e6df8db6-9384-46ed-864a-7ad8d06025ee] Running
I0104 13:36:24.627099  608550 system_pods.go:61] "kube-proxy-2sfhm" [6cda94e1-6c04-4d0e-b0ad-a1ebe6b5399a] Running
I0104 13:36:24.627101  608550 system_pods.go:61] "kube-proxy-5nxwg" [0b493108-b17e-4cbc-9e8c-fd0a1893929f] Running
I0104 13:36:24.627104  608550 system_pods.go:61] "kube-proxy-74g9h" [e73297c2-cb81-46b2-a166-bbc0548611ef] Running
I0104 13:36:24.627106  608550 system_pods.go:61] "kube-scheduler-k8s-node-cassandra" [0c3efdd6-96ab-4976-8f08-97d02ced1668] Running
I0104 13:36:24.627109  608550 system_pods.go:61] "storage-provisioner" [7eb5c2a2-e718-4b96-a490-7659d8e00ed2] Running
I0104 13:36:24.627111  608550 system_pods.go:74] duration metric: took 7.149635ms to wait for pod list to return data ...
I0104 13:36:24.627116  608550 node_conditions.go:102] verifying NodePressure condition ...
I0104 13:36:24.629098  608550 node_conditions.go:122] node storage ephemeral capacity is 240785624Ki
I0104 13:36:24.629108  608550 node_conditions.go:123] node cpu capacity is 8
I0104 13:36:24.629114  608550 node_conditions.go:122] node storage ephemeral capacity is 240785624Ki
I0104 13:36:24.629117  608550 node_conditions.go:123] node cpu capacity is 8
I0104 13:36:24.629119  608550 node_conditions.go:122] node storage ephemeral capacity is 240785624Ki
I0104 13:36:24.629121  608550 node_conditions.go:123] node cpu capacity is 8
I0104 13:36:24.629123  608550 node_conditions.go:105] duration metric: took 2.00498ms to run NodePressure ...
I0104 13:36:24.629133  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0104 13:36:24.716765  608550 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0104 13:36:24.721202  608550 ops.go:34] apiserver oom_adj: -16
I0104 13:36:24.721210  608550 kubeadm.go:631] restartCluster took 10.367764395s
I0104 13:36:24.721215  608550 kubeadm.go:398] StartCluster complete in 10.401434672s
I0104 13:36:24.721225  608550 settings.go:142] acquiring lock: {Name:mkdaa8ebbcc50d075c74f590f3b451982e493801 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 13:36:24.721281  608550 settings.go:150] Updating kubeconfig:  /home/brunopec/.kube/config
I0104 13:36:24.721653  608550 lock.go:35] WriteFile acquiring /home/brunopec/.kube/config: {Name:mk5d9277b23335389bc614973afcf1589505ba9b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0104 13:36:24.723395  608550 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "k8s-node-cassandra" rescaled to 1
I0104 13:36:24.723428  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.2/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0104 13:36:24.723426  608550 start.go:212] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true}
I0104 13:36:24.724622  608550 out.go:177] 🔎  Verifying Kubernetes components...
I0104 13:36:24.723469  608550 addons.go:412] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0104 13:36:24.724646  608550 addons.go:65] Setting storage-provisioner=true in profile "k8s-node-cassandra"
I0104 13:36:24.725690  608550 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0104 13:36:24.723582  608550 config.go:180] Loaded profile config "k8s-node-cassandra": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I0104 13:36:24.724651  608550 addons.go:65] Setting default-storageclass=true in profile "k8s-node-cassandra"
I0104 13:36:24.725689  608550 addons.go:153] Setting addon storage-provisioner=true in "k8s-node-cassandra"
I0104 13:36:24.725728  608550 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "k8s-node-cassandra"
W0104 13:36:24.725729  608550 addons.go:162] addon storage-provisioner should already be in state true
I0104 13:36:24.725766  608550 host.go:66] Checking if "k8s-node-cassandra" exists ...
I0104 13:36:24.724660  608550 addons.go:65] Setting ingress=true in profile "k8s-node-cassandra"
I0104 13:36:24.725798  608550 addons.go:153] Setting addon ingress=true in "k8s-node-cassandra"
W0104 13:36:24.725803  608550 addons.go:162] addon ingress should already be in state true
I0104 13:36:24.725831  608550 host.go:66] Checking if "k8s-node-cassandra" exists ...
I0104 13:36:24.725941  608550 cli_runner.go:164] Run: docker container inspect k8s-node-cassandra --format={{.State.Status}}
I0104 13:36:24.726049  608550 cli_runner.go:164] Run: docker container inspect k8s-node-cassandra --format={{.State.Status}}
I0104 13:36:24.726092  608550 cli_runner.go:164] Run: docker container inspect k8s-node-cassandra --format={{.State.Status}}
I0104 13:36:24.754320  608550 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0104 13:36:24.755838  608550 out.go:177]     ▪ Using image k8s.gcr.io/ingress-nginx/controller:v1.2.1
I0104 13:36:24.755580  608550 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0104 13:36:24.757120  608550 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0104 13:36:24.758222  608550 out.go:177]     ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0104 13:36:24.757190  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:24.760597  608550 out.go:177]     ▪ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0104 13:36:24.760090  608550 addons.go:153] Setting addon default-storageclass=true in "k8s-node-cassandra"
W0104 13:36:24.761804  608550 addons.go:162] addon default-storageclass should already be in state true
I0104 13:36:24.761822  608550 host.go:66] Checking if "k8s-node-cassandra" exists ...
I0104 13:36:24.761874  608550 addons.go:345] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0104 13:36:24.761881  608550 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (15567 bytes)
I0104 13:36:24.761922  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:24.762085  608550 cli_runner.go:164] Run: docker container inspect k8s-node-cassandra --format={{.State.Status}}
I0104 13:36:24.793283  608550 start.go:806] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0104 13:36:24.793283  608550 api_server.go:51] waiting for apiserver process to appear ...
I0104 13:36:24.793356  608550 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0104 13:36:24.793752  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:24.794629  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:24.794766  608550 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I0104 13:36:24.794773  608550 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0104 13:36:24.794816  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:24.803299  608550 api_server.go:71] duration metric: took 79.852682ms to wait for apiserver process to appear ...
I0104 13:36:24.803326  608550 api_server.go:87] waiting for apiserver healthz status ...
I0104 13:36:24.803338  608550 api_server.go:252] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0104 13:36:24.807441  608550 api_server.go:278] https://192.168.58.2:8443/healthz returned 200:
ok
I0104 13:36:24.807891  608550 api_server.go:140] control plane version: v1.25.2
I0104 13:36:24.807898  608550 api_server.go:130] duration metric: took 4.568861ms to wait for apiserver health ...
I0104 13:36:24.807902  608550 system_pods.go:43] waiting for kube-system pods to appear ...
I0104 13:36:24.815722  608550 system_pods.go:59] 16 kube-system pods found
I0104 13:36:24.815744  608550 system_pods.go:61] "coredns-565d847f94-rqqr2" [2c299796-87a7-48fb-b68d-20f55c4fa07d] Running
I0104 13:36:24.815748  608550 system_pods.go:61] "csi-nfs-controller-679b6d5754-lzrjk" [03ff925d-e48d-46bf-8fa0-5299f446b61c] Running
I0104 13:36:24.815751  608550 system_pods.go:61] "csi-nfs-node-2n65m" [ff7d8c2e-c2fa-465a-894c-4922ac01b8d1] Running
I0104 13:36:24.815753  608550 system_pods.go:61] "csi-nfs-node-fzbkq" [54cc4b6a-2718-4c46-b33d-fda54b106d70] Running
I0104 13:36:24.815756  608550 system_pods.go:61] "csi-nfs-node-qh9kd" [93ad38d6-6476-4a84-bbec-61594c1ac28d] Running
I0104 13:36:24.815758  608550 system_pods.go:61] "etcd-k8s-node-cassandra" [6bc29607-e51f-47f2-b5ff-c6488844dc17] Running
I0104 13:36:24.815760  608550 system_pods.go:61] "kindnet-6sckd" [3561a674-4aba-431c-9a21-3d607358be50] Running
I0104 13:36:24.815763  608550 system_pods.go:61] "kindnet-hkj5s" [650f220e-ba2e-4bf4-86db-f1517dee4de5] Running
I0104 13:36:24.815765  608550 system_pods.go:61] "kindnet-zxfwt" [4b4f869e-2080-47cc-94f0-6a4102f75be7] Running
I0104 13:36:24.815771  608550 system_pods.go:61] "kube-apiserver-k8s-node-cassandra" [c8776e9a-597d-47c1-9495-6e4b6dedc1c9] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0104 13:36:24.815777  608550 system_pods.go:61] "kube-controller-manager-k8s-node-cassandra" [e6df8db6-9384-46ed-864a-7ad8d06025ee] Running
I0104 13:36:24.815781  608550 system_pods.go:61] "kube-proxy-2sfhm" [6cda94e1-6c04-4d0e-b0ad-a1ebe6b5399a] Running
I0104 13:36:24.815784  608550 system_pods.go:61] "kube-proxy-5nxwg" [0b493108-b17e-4cbc-9e8c-fd0a1893929f] Running
I0104 13:36:24.815787  608550 system_pods.go:61] "kube-proxy-74g9h" [e73297c2-cb81-46b2-a166-bbc0548611ef] Running
I0104 13:36:24.815791  608550 system_pods.go:61] "kube-scheduler-k8s-node-cassandra" [0c3efdd6-96ab-4976-8f08-97d02ced1668] Running
I0104 13:36:24.815796  608550 system_pods.go:61] "storage-provisioner" [7eb5c2a2-e718-4b96-a490-7659d8e00ed2] Running
I0104 13:36:24.815805  608550 system_pods.go:74] duration metric: took 7.894928ms to wait for pod list to return data ...
I0104 13:36:24.815818  608550 kubeadm.go:573] duration metric: took 92.376753ms to wait for : map[apiserver:true system_pods:true] ...
I0104 13:36:24.815834  608550 node_conditions.go:102] verifying NodePressure condition ...
I0104 13:36:24.816008  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:24.818439  608550 node_conditions.go:122] node storage ephemeral capacity is 240785624Ki
I0104 13:36:24.818450  608550 node_conditions.go:123] node cpu capacity is 8
I0104 13:36:24.818457  608550 node_conditions.go:122] node storage ephemeral capacity is 240785624Ki
I0104 13:36:24.818459  608550 node_conditions.go:123] node cpu capacity is 8
I0104 13:36:24.818461  608550 node_conditions.go:122] node storage ephemeral capacity is 240785624Ki
I0104 13:36:24.818464  608550 node_conditions.go:123] node cpu capacity is 8
I0104 13:36:24.818466  608550 node_conditions.go:105] duration metric: took 2.629218ms to run NodePressure ...
I0104 13:36:24.818473  608550 start.go:217] waiting for startup goroutines ...
I0104 13:36:24.883841  608550 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0104 13:36:24.884728  608550 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0104 13:36:24.909154  608550 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0104 13:36:25.137017  608550 addons.go:383] Verifying addon ingress=true in "k8s-node-cassandra"
I0104 13:36:25.138824  608550 out.go:177] 🔎  Verifying ingress addon...
I0104 13:36:25.140823  608550 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0104 13:36:25.144157  608550 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0104 13:36:25.144167  608550 kapi.go:108] duration metric: took 3.347727ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0104 13:36:25.145574  608550 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass, ingress
I0104 13:36:25.146797  608550 addons.go:414] enableAddons completed in 423.319213ms
I0104 13:36:25.147403  608550 config.go:180] Loaded profile config "k8s-node-cassandra": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I0104 13:36:25.147529  608550 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I0104 13:36:25.147619  608550 profile.go:148] Saving config to /home/brunopec/.minikube/profiles/k8s-node-cassandra/config.json ...
I0104 13:36:25.149784  608550 out.go:177] 👍  Starting worker node k8s-node-cassandra-m02 in cluster k8s-node-cassandra
I0104 13:36:25.151103  608550 cache.go:120] Beginning downloading kic base image for docker with docker
I0104 13:36:25.152236  608550 out.go:177] 🚜  Pulling base image ...
I0104 13:36:25.153438  608550 preload.go:132] Checking if preload exists for k8s version v1.25.2 and runtime docker
I0104 13:36:25.153472  608550 cache.go:57] Caching tarball of preloaded images
I0104 13:36:25.153521  608550 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b in local docker daemon
I0104 13:36:25.153650  608550 preload.go:174] Found /home/brunopec/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.2-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0104 13:36:25.153676  608550 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.2 on docker
I0104 13:36:25.153827  608550 profile.go:148] Saving config to /home/brunopec/.minikube/profiles/k8s-node-cassandra/config.json ...
I0104 13:36:25.199244  608550 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b in local docker daemon, skipping pull
I0104 13:36:25.199269  608550 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b exists in daemon, skipping load
I0104 13:36:25.199289  608550 cache.go:208] Successfully downloaded all kic artifacts
I0104 13:36:25.199333  608550 start.go:364] acquiring machines lock for k8s-node-cassandra-m02: {Name:mk1af7c4bb0eba9040a1c3ad1580afeffa18b8b5 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0104 13:36:25.199450  608550 start.go:368] acquired machines lock for "k8s-node-cassandra-m02" in 101.239µs
I0104 13:36:25.199468  608550 start.go:96] Skipping create...Using existing machine configuration
I0104 13:36:25.199473  608550 fix.go:55] fixHost starting: m02
I0104 13:36:25.199688  608550 cli_runner.go:164] Run: docker container inspect k8s-node-cassandra-m02 --format={{.State.Status}}
I0104 13:36:25.219232  608550 fix.go:103] recreateIfNeeded on k8s-node-cassandra-m02: state=Running err=<nil>
W0104 13:36:25.219247  608550 fix.go:129] unexpected machine state, will restart: <nil>
I0104 13:36:25.220905  608550 out.go:177] 🏃  Updating the running docker "k8s-node-cassandra-m02" container ...
I0104 13:36:25.222027  608550 machine.go:88] provisioning docker machine ...
I0104 13:36:25.222041  608550 ubuntu.go:169] provisioning hostname "k8s-node-cassandra-m02"
I0104 13:36:25.222083  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:25.266006  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:36:25.266142  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49167 <nil> <nil>}
I0104 13:36:25.266152  608550 main.go:134] libmachine: About to run SSH command:
sudo hostname k8s-node-cassandra-m02 && echo "k8s-node-cassandra-m02" | sudo tee /etc/hostname
I0104 13:36:25.394510  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: k8s-node-cassandra-m02

I0104 13:36:25.394570  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:25.412559  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:36:25.412673  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49167 <nil> <nil>}
I0104 13:36:25.412686  608550 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sk8s-node-cassandra-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 k8s-node-cassandra-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 k8s-node-cassandra-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0104 13:36:25.527524  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0104 13:36:25.527537  608550 ubuntu.go:175] set auth options {CertDir:/home/brunopec/.minikube CaCertPath:/home/brunopec/.minikube/certs/ca.pem CaPrivateKeyPath:/home/brunopec/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/brunopec/.minikube/machines/server.pem ServerKeyPath:/home/brunopec/.minikube/machines/server-key.pem ClientKeyPath:/home/brunopec/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/brunopec/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/brunopec/.minikube}
I0104 13:36:25.527548  608550 ubuntu.go:177] setting up certificates
I0104 13:36:25.527553  608550 provision.go:83] configureAuth start
I0104 13:36:25.527601  608550 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" k8s-node-cassandra-m02
I0104 13:36:25.544309  608550 provision.go:138] copyHostCerts
I0104 13:36:25.544343  608550 exec_runner.go:144] found /home/brunopec/.minikube/ca.pem, removing ...
I0104 13:36:25.544351  608550 exec_runner.go:207] rm: /home/brunopec/.minikube/ca.pem
I0104 13:36:25.544416  608550 exec_runner.go:151] cp: /home/brunopec/.minikube/certs/ca.pem --> /home/brunopec/.minikube/ca.pem (1082 bytes)
I0104 13:36:25.544469  608550 exec_runner.go:144] found /home/brunopec/.minikube/cert.pem, removing ...
I0104 13:36:25.544472  608550 exec_runner.go:207] rm: /home/brunopec/.minikube/cert.pem
I0104 13:36:25.544486  608550 exec_runner.go:151] cp: /home/brunopec/.minikube/certs/cert.pem --> /home/brunopec/.minikube/cert.pem (1127 bytes)
I0104 13:36:25.544535  608550 exec_runner.go:144] found /home/brunopec/.minikube/key.pem, removing ...
I0104 13:36:25.544537  608550 exec_runner.go:207] rm: /home/brunopec/.minikube/key.pem
I0104 13:36:25.544561  608550 exec_runner.go:151] cp: /home/brunopec/.minikube/certs/key.pem --> /home/brunopec/.minikube/key.pem (1679 bytes)
I0104 13:36:25.544581  608550 provision.go:112] generating server cert: /home/brunopec/.minikube/machines/server.pem ca-key=/home/brunopec/.minikube/certs/ca.pem private-key=/home/brunopec/.minikube/certs/ca-key.pem org=brunopec.k8s-node-cassandra-m02 san=[192.168.58.3 127.0.0.1 localhost 127.0.0.1 minikube k8s-node-cassandra-m02]
I0104 13:36:25.609291  608550 provision.go:172] copyRemoteCerts
I0104 13:36:25.609339  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0104 13:36:25.609376  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:25.632642  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49167 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra-m02/id_rsa Username:docker}
I0104 13:36:25.722242  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0104 13:36:25.738468  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/machines/server.pem --> /etc/docker/server.pem (1245 bytes)
I0104 13:36:25.756573  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0104 13:36:25.783694  608550 provision.go:86] duration metric: configureAuth took 256.12959ms
I0104 13:36:25.783712  608550 ubuntu.go:193] setting minikube options for container-runtime
I0104 13:36:25.783900  608550 config.go:180] Loaded profile config "k8s-node-cassandra": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I0104 13:36:25.783963  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:25.813650  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:36:25.813961  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49167 <nil> <nil>}
I0104 13:36:25.813970  608550 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0104 13:36:25.941599  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0104 13:36:25.941612  608550 ubuntu.go:71] root file system type: overlay
I0104 13:36:25.941756  608550 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0104 13:36:25.941819  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:25.985744  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:36:25.985990  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49167 <nil> <nil>}
I0104 13:36:25.986050  608550 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.58.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0104 13:36:26.135604  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.58.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0104 13:36:26.135676  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:26.158068  608550 main.go:134] libmachine: Using SSH client type: native
I0104 13:36:26.158185  608550 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49167 <nil> <nil>}
I0104 13:36:26.158200  608550 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0104 13:36:26.277443  608550 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0104 13:36:26.277459  608550 machine.go:91] provisioned docker machine in 1.055425299s
I0104 13:36:26.277468  608550 start.go:300] post-start starting for "k8s-node-cassandra-m02" (driver="docker")
I0104 13:36:26.277475  608550 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0104 13:36:26.277519  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0104 13:36:26.277553  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:26.292942  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49167 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra-m02/id_rsa Username:docker}
I0104 13:36:26.377378  608550 ssh_runner.go:195] Run: cat /etc/os-release
I0104 13:36:26.379257  608550 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0104 13:36:26.379273  608550 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0104 13:36:26.379279  608550 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0104 13:36:26.379284  608550 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0104 13:36:26.379292  608550 filesync.go:126] Scanning /home/brunopec/.minikube/addons for local assets ...
I0104 13:36:26.379322  608550 filesync.go:126] Scanning /home/brunopec/.minikube/files for local assets ...
I0104 13:36:26.379331  608550 start.go:303] post-start completed in 101.857399ms
I0104 13:36:26.379368  608550 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0104 13:36:26.379392  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:26.396149  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49167 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra-m02/id_rsa Username:docker}
I0104 13:36:26.474993  608550 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0104 13:36:26.477789  608550 fix.go:57] fixHost completed within 1.278310513s
I0104 13:36:26.477804  608550 start.go:83] releasing machines lock for "k8s-node-cassandra-m02", held for 1.278346709s
I0104 13:36:26.477874  608550 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" k8s-node-cassandra-m02
I0104 13:36:26.494010  608550 out.go:177] 🌐  Found network options:
I0104 13:36:26.495355  608550 out.go:177]     ▪ NO_PROXY=192.168.58.2
W0104 13:36:26.496416  608550 proxy.go:119] fail to check proxy env: Error ip not in block
W0104 13:36:26.496440  608550 proxy.go:119] fail to check proxy env: Error ip not in block
I0104 13:36:26.496526  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0104 13:36:26.496554  608550 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0104 13:36:26.496563  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:26.496600  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra-m02
I0104 13:36:26.525094  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49167 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra-m02/id_rsa Username:docker}
I0104 13:36:26.525856  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49167 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra-m02/id_rsa Username:docker}
I0104 13:36:26.610074  608550 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (234 bytes)
I0104 13:36:26.621299  608550 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 13:36:27.327860  608550 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0104 13:36:27.387299  608550 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0104 13:36:27.394874  608550 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0104 13:36:27.394913  608550 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0104 13:36:27.402227  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0104 13:36:27.417595  608550 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0104 13:36:27.513418  608550 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0104 13:36:27.633323  608550 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 13:36:27.725989  608550 ssh_runner.go:195] Run: sudo systemctl restart docker
I0104 13:36:38.151392  608550 ssh_runner.go:235] Completed: sudo systemctl restart docker: (10.42537953s)
I0104 13:36:38.151457  608550 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0104 13:36:38.276670  608550 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0104 13:36:38.406065  608550 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0104 13:36:38.413999  608550 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0104 13:36:38.414049  608550 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0104 13:36:38.416247  608550 start.go:472] Will wait 60s for crictl version
I0104 13:36:38.416290  608550 ssh_runner.go:195] Run: sudo crictl version
I0104 13:36:38.444294  608550 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.18
RuntimeApiVersion:  1.41.0
I0104 13:36:38.444338  608550 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 13:36:38.477624  608550 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0104 13:36:38.498980  608550 out.go:204] 🐳  Preparing Kubernetes v1.25.2 on Docker 20.10.18 ...
I0104 13:36:38.500631  608550 out.go:177]     ▪ env NO_PROXY=192.168.58.2
I0104 13:36:38.501474  608550 cli_runner.go:164] Run: docker network inspect k8s-node-cassandra --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0104 13:36:38.519531  608550 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0104 13:36:38.521790  608550 certs.go:54] Setting up /home/brunopec/.minikube/profiles/k8s-node-cassandra for IP: 192.168.58.3
I0104 13:36:38.521852  608550 certs.go:182] skipping minikubeCA CA generation: /home/brunopec/.minikube/ca.key
I0104 13:36:38.521869  608550 certs.go:182] skipping proxyClientCA CA generation: /home/brunopec/.minikube/proxy-client-ca.key
I0104 13:36:38.521911  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/ca-key.pem (1675 bytes)
I0104 13:36:38.521925  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/ca.pem (1082 bytes)
I0104 13:36:38.521945  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/cert.pem (1127 bytes)
I0104 13:36:38.521957  608550 certs.go:388] found cert: /home/brunopec/.minikube/certs/home/brunopec/.minikube/certs/key.pem (1679 bytes)
I0104 13:36:38.522596  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0104 13:36:38.535485  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0104 13:36:38.547636  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0104 13:36:38.558042  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0104 13:36:38.568297  608550 ssh_runner.go:362] scp /home/brunopec/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0104 13:36:38.579397  608550 ssh_runner.go:195] Run: openssl version
I0104 13:36:38.582216  608550 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0104 13:36:38.586761  608550 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0104 13:36:38.588482  608550 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Nov  1 21:30 /usr/share/ca-certificates/minikubeCA.pem
I0104 13:36:38.588514  608550 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0104 13:36:38.591472  608550 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0104 13:36:38.595622  608550 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0104 13:36:38.637414  608550 cni.go:95] Creating CNI manager for ""
I0104 13:36:38.637420  608550 cni.go:156] 3 nodes found, recommending kindnet
I0104 13:36:38.637427  608550 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0104 13:36:38.637435  608550 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.3 APIServerPort:8443 KubernetesVersion:v1.25.2 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:k8s-node-cassandra NodeName:k8s-node-cassandra-m02 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.3 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I0104 13:36:38.637503  608550 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "k8s-node-cassandra-m02"
  kubeletExtraArgs:
    node-ip: 192.168.58.3
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.2
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0104 13:36:38.637536  608550 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.2/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=k8s-node-cassandra-m02 --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.3 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.2 ClusterName:k8s-node-cassandra Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0104 13:36:38.637573  608550 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.2
I0104 13:36:38.641557  608550 binaries.go:44] Found k8s binaries, skipping transfer
I0104 13:36:38.641593  608550 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0104 13:36:38.645727  608550 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (484 bytes)
I0104 13:36:38.653340  608550 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0104 13:36:38.661025  608550 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0104 13:36:38.662787  608550 host.go:66] Checking if "k8s-node-cassandra" exists ...
I0104 13:36:38.662984  608550 config.go:180] Loaded profile config "k8s-node-cassandra": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I0104 13:36:38.662980  608550 start.go:286] JoinCluster: &{Name:k8s-node-cassandra KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.2 ClusterName:k8s-node-cassandra Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.58.4 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/brunopec:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I0104 13:36:38.663049  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm token create --print-join-command --ttl=0"
I0104 13:36:38.663083  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:38.676708  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:38.789843  608550 start.go:299] removing existing worker node "m02" before attempting to rejoin cluster: &{Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true}
I0104 13:36:38.789867  608550 host.go:66] Checking if "k8s-node-cassandra" exists ...
I0104 13:36:38.790026  608550 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl drain k8s-node-cassandra-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data
I0104 13:36:38.790051  608550 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" k8s-node-cassandra
I0104 13:36:38.805686  608550 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/brunopec/.minikube/machines/k8s-node-cassandra/id_rsa Username:docker}
I0104 13:36:41.994154  608550 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl drain k8s-node-cassandra-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data: (3.20411242s)
I0104 13:36:41.994164  608550 node.go:109] successfully drained node "m02"
I0104 13:36:41.996560  608550 node.go:125] successfully deleted node "m02"
I0104 13:36:41.996574  608550 start.go:303] successfully removed existing worker node "m02" from cluster: &{Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true}
I0104 13:36:41.996591  608550 start.go:307] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.58.3 Port:0 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:false Worker:true}
I0104 13:36:41.996605  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm join control-plane.minikube.internal:8443 --token oib1iu.otq39qs6slrkophh --discovery-token-ca-cert-hash sha256:435afe3aab5ed06140d378815587507dd1fecf492439cade8364e330c63bf932 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=k8s-node-cassandra-m02"
I0104 13:41:42.094675  608550 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm join control-plane.minikube.internal:8443 --token oib1iu.otq39qs6slrkophh --discovery-token-ca-cert-hash sha256:435afe3aab5ed06140d378815587507dd1fecf492439cade8364e330c63bf932 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=k8s-node-cassandra-m02": (5m0.098056568s)
E0104 13:41:42.094704  608550 start.go:309] worker node failed to join cluster, will retry: kubeadm join: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm join control-plane.minikube.internal:8443 --token oib1iu.otq39qs6slrkophh --discovery-token-ca-cert-hash sha256:435afe3aab5ed06140d378815587507dd1fecf492439cade8364e330c63bf932 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=k8s-node-cassandra-m02": Process exited with status 1
stdout:
[preflight] Running pre-flight checks

stderr:
W0104 16:36:42.015339   14806 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING SystemVerification]: missing optional cgroups: blkio
	[WARNING Port-10250]: Port 10250 is in use
	[WARNING FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
error execution phase preflight: couldn't validate the identity of the API Server: illegal base64 data at input byte 3
To see the stack trace of this error execute with --v=5 or higher
I0104 13:41:42.094723  608550 start.go:312] resetting worker node "m02" before attempting to rejoin cluster...
I0104 13:41:42.094730  608550 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm reset --force"
I0104 13:41:42.120049  608550 start.go:314] kubeadm reset failed, continuing anyway: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm reset --force": Process exited with status 1
stdout:

stderr:
Found multiple CRI endpoints on the host. Please define which one do you wish to use by setting the 'criSocket' field in the kubeadm configuration file: unix:///var/run/containerd/containerd.sock, unix:///var/run/cri-dockerd.sock
To see the stack trace of this error execute with --v=5 or higher
I0104 13:41:42.120074  608550 start.go:288] JoinCluster complete in 5m3.457094444s
I0104 13:41:42.121666  608550 out.go:177] 
W0104 13:41:42.122574  608550 out.go:239] ❌  Exiting due to GUEST_START: adding node: joining cp: error joining worker node to cluster: kubeadm join: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm join control-plane.minikube.internal:8443 --token oib1iu.otq39qs6slrkophh --discovery-token-ca-cert-hash sha256:435afe3aab5ed06140d378815587507dd1fecf492439cade8364e330c63bf932 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=k8s-node-cassandra-m02": Process exited with status 1
stdout:
[preflight] Running pre-flight checks

stderr:
W0104 16:36:42.015339   14806 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING SystemVerification]: missing optional cgroups: blkio
	[WARNING Port-10250]: Port 10250 is in use
	[WARNING FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
error execution phase preflight: couldn't validate the identity of the API Server: illegal base64 data at input byte 3
To see the stack trace of this error execute with --v=5 or higher

W0104 13:41:42.122596  608550 out.go:239] 
W0104 13:41:42.123118  608550 out.go:239] [31m╭───────────────────────────────────────────────────────────────────────────────────────────╮[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    😿  If the above advice does not help, please let us know:                             [31m│[0m
[31m│[0m    👉  https://github.com/kubernetes/minikube/issues/new/choose                           [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯[0m
I0104 13:41:42.125314  608550 out.go:177] 

* 
* ==> Docker <==
* -- Logs begin at Wed 2023-01-04 16:25:34 UTC, end at Wed 2023-01-04 16:43:57 UTC. --
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.376391841Z" level=warning msg="Not using native diff for overlay2, this may cause degraded performance for building images: kernel has CONFIG_OVERLAY_FS_REDIRECT_DIR enabled" storage-driver=overlay2
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.376718264Z" level=info msg="Docker daemon" commit=e42327a graphdriver(s)=overlay2 version=20.10.18
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.376794272Z" level=info msg="Daemon has completed initialization"
Jan 04 16:36:12 k8s-node-cassandra systemd[1]: Started Docker Application Container Engine.
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.387967155Z" level=info msg="API listen on [::]:2376"
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.391490271Z" level=info msg="API listen on /var/run/docker.sock"
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.549891174Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.575059103Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.715202628Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:12 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:12.783088985Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.554192509Z" level=info msg="ignoring event" container=667442ea6481b3965b4c06e623e03488bb851766f3e771394414949c9870c875 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.559813993Z" level=info msg="ignoring event" container=5c4c76e6b7bf0fa6dfb7d5e5698c2e4464aef2e25d4052d81b1e6331e5fed45a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.560323371Z" level=info msg="ignoring event" container=80219953857a33c97aca28627c785731145f4d1642948898f3894ca7a4912f03 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.560368038Z" level=info msg="ignoring event" container=b36cf481d59ac0d39b84b268e723f56394f3ae1ce2054e901f6ff781935d2b85 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.560384565Z" level=info msg="ignoring event" container=ba955d8a3949842302cba845997af8eda46dd77f785cc7e8b708b415f2b24cf1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.560398116Z" level=info msg="ignoring event" container=ecbb48f8e31bf69270b1604fb787e32e519c204071f53e2f1a884aa763350add module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.568238041Z" level=info msg="ignoring event" container=7f2aecf5d1919cc0b018420bb8bf3f1e886af31ffe5f6b3dd088d7983c2e6787 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.597435935Z" level=info msg="ignoring event" container=7b19508f34bfa0499d7db184e1713cbf5d268ee4867d80b16cf27d2f0772e3ce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.647351364Z" level=info msg="ignoring event" container=c4d6364a88ed0af8ca0c160bc85c3e514a4c1b045072c0ad223567d27137043f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.698531712Z" level=info msg="ignoring event" container=fa026835d8b9c549668a18c6307032479d7ca20269f39f31030182e6728607cb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.711489353Z" level=info msg="ignoring event" container=f8d8ab2a9624bbd6fea0241476f3bb48a883c5c2ddc482cb8c08fb766c986c76 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.721380576Z" level=info msg="ignoring event" container=db01d9e60bd65147cefcc3c1cb041301c1190774685befed3103a80aef78e918 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.735365003Z" level=info msg="ignoring event" container=bc617017f7655e1ec8912319b1f9eb98ebd4833fc21ca24e4f777a0bb047ccfe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.735399332Z" level=info msg="ignoring event" container=eb1fc7afba15926d44f4f936eca4b2e46e47b365c14d30a9b35d2b97cb691713 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.748917530Z" level=info msg="ignoring event" container=d80f8b672c04c30682ae4fa0944d08df5e2f509bb6b86e91bee4efe3f8d526a3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.792663394Z" level=error msg="5a0e7f93408a8e45f3684e751a6bec7f7f771e0f21b1afa95d3b45489b80b957 cleanup: failed to delete container from containerd: no such container"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.797473737Z" level=error msg="0970992bc3cb3263487d7cad3a67d9c964c14fc6b8cf854e7ed8a99b6d7f7e6b cleanup: failed to delete container from containerd: no such container"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.816266912Z" level=error msg="bb85968a4174df3b3423380e510a75772a7c6166e6eed26ec52338bc21addcb7 cleanup: failed to delete container from containerd: no such container"
Jan 04 16:36:17 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:17.823667727Z" level=error msg="26c0bc4ccaa4da85e8abc9723e7b72b1a683f840a40c528af997f660bd94eb40 cleanup: failed to delete container from containerd: no such container"
Jan 04 16:36:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:26.082274707Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:26.104942649Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:27 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:27.220033098Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:27 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:27.236835267Z" level=warning msg="Published ports are discarded when using host network mode"
Jan 04 16:36:37 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:37.929949175Z" level=info msg="ignoring event" container=8a87d659b1169df2a8fa5d3017e17c70f42d48ae1457b4d18e062fe2dbf736e4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:36:52 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:36:52.416453990Z" level=info msg="ignoring event" container=472a9aa51e4a0d771e9840c6793bc7cbb5b53617e250db1e75d421cd97063133 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:37:08 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:37:08.865615947Z" level=info msg="ignoring event" container=5c3febdc26268adb8afd5c81a69877f0dba234e0eec378eb35ec6a8e1045e5a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:37:22 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:37:22.634342919Z" level=info msg="ignoring event" container=3c028e7add5ab838851b0e8187fdc3b07a9306770f30dd56ebcd34b785d5d220 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:37:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:37:26.811713536Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=aa94f8c679ee2089cb19092f2cd481ae2a9a22ace8074be401290142a3a1f9fb
Jan 04 16:37:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:37:26.889689223Z" level=info msg="ignoring event" container=aa94f8c679ee2089cb19092f2cd481ae2a9a22ace8074be401290142a3a1f9fb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:38:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:38:26.810983639Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=03b288a4a09257f66ec3dbf059e648d2286d9d20135b11f6b8c308891a54c332
Jan 04 16:38:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:38:26.887897192Z" level=info msg="ignoring event" container=03b288a4a09257f66ec3dbf059e648d2286d9d20135b11f6b8c308891a54c332 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:38:38 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:38:38.516767827Z" level=error msg="Not continuing with pull after error: context canceled"
Jan 04 16:38:38 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:38:38.538505160Z" level=info msg="Layer sha256:580c8ff353b07bfb6c3639dcd8c41fb5b2489b57b4696bfe5f8a94899618709a cleaned up"
Jan 04 16:38:38 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:38:38.538527172Z" level=info msg="Layer sha256:7a1c78faa637e0f35540408a0de3e1b6108c61c2ae7c9a82bd1bde5c7064f2e9 cleaned up"
Jan 04 16:38:38 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:38:38.610591738Z" level=info msg="Layer sha256:870a241bfebd02ea6be466c73f4484cba43d9253a075894c2f23a39aa5cca005 cleaned up"
Jan 04 16:39:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:39:26.813099939Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=26cc31bcb38b91d497bc6749711e9791b45b7a894cd11a6df524773c914ed57a
Jan 04 16:39:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:39:26.885679642Z" level=info msg="ignoring event" container=26cc31bcb38b91d497bc6749711e9791b45b7a894cd11a6df524773c914ed57a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:40:26 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:40:26.953706101Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0
Jan 04 16:40:27 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:40:27.019229174Z" level=info msg="ignoring event" container=1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:40:49 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:40:49.772483188Z" level=error msg="Not continuing with pull after error: context canceled"
Jan 04 16:40:49 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:40:49.905036285Z" level=info msg="Layer sha256:8c472c55e0fbed2391f155caa6297d64d9665803c55f9b6719d7c17abf1e0459 cleaned up"
Jan 04 16:40:49 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:40:49.924244076Z" level=info msg="Layer sha256:580c8ff353b07bfb6c3639dcd8c41fb5b2489b57b4696bfe5f8a94899618709a cleaned up"
Jan 04 16:40:49 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:40:49.924264281Z" level=info msg="Layer sha256:7a1c78faa637e0f35540408a0de3e1b6108c61c2ae7c9a82bd1bde5c7064f2e9 cleaned up"
Jan 04 16:40:50 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:40:50.002363566Z" level=info msg="Layer sha256:870a241bfebd02ea6be466c73f4484cba43d9253a075894c2f23a39aa5cca005 cleaned up"
Jan 04 16:42:56 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:42:56.809929690Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=1a1ac21e6f83d8632f7f87cfa08ae61258e4ca302eb361792bc708ab166cf451
Jan 04 16:42:56 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:42:56.880140706Z" level=info msg="ignoring event" container=1a1ac21e6f83d8632f7f87cfa08ae61258e4ca302eb361792bc708ab166cf451 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 04 16:43:16 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:43:16.767942960Z" level=error msg="Not continuing with pull after error: context canceled"
Jan 04 16:43:16 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:43:16.778650552Z" level=info msg="Layer sha256:580c8ff353b07bfb6c3639dcd8c41fb5b2489b57b4696bfe5f8a94899618709a cleaned up"
Jan 04 16:43:16 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:43:16.787978091Z" level=info msg="Layer sha256:7a1c78faa637e0f35540408a0de3e1b6108c61c2ae7c9a82bd1bde5c7064f2e9 cleaned up"
Jan 04 16:43:16 k8s-node-cassandra dockerd[22540]: time="2023-01-04T16:43:16.858030242Z" level=info msg="Layer sha256:870a241bfebd02ea6be466c73f4484cba43d9253a075894c2f23a39aa5cca005 cleaned up"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                       CREATED              STATE               NAME                      ATTEMPT             POD ID
1a1ac21e6f83d       75bdf78d9d67e                                                                               About a minute ago   Exited              controller                12                  27607bce4526d
960ea1c50a9b0       611451465cbc2                                                                               6 minutes ago        Running             nginx                     3                   56b8c7efd47ef
64c117b13294d       f24d8135658e9                                                                               6 minutes ago        Running             git-repo-syncer           4                   56b8c7efd47ef
2610eac88441f       611451465cbc2                                                                               6 minutes ago        Running             nginx                     0                   2acc34c6a227d
222c8e8ac24e0       f24d8135658e9                                                                               6 minutes ago        Running             git-repo-syncer           0                   2acc34c6a227d
e1cdaa0098cb2       f24d8135658e9                                                                               7 minutes ago        Running             git-repo-syncer           0                   d3ae5963b9d28
5315a02ae4800       611451465cbc2                                                                               7 minutes ago        Running             nginx                     0                   d3ae5963b9d28
5c3febdc26268       f24d8135658e9                                                                               7 minutes ago        Exited              git-clone-repository      0                   2acc34c6a227d
472a9aa51e4a0       f24d8135658e9                                                                               7 minutes ago        Exited              git-clone-repository      0                   d3ae5963b9d28
3c028e7add5ab       f24d8135658e9                                                                               7 minutes ago        Exited              git-clone-repository      1                   56b8c7efd47ef
bc205a22fd0bf       d6e3e26021b60                                                                               7 minutes ago        Running             kindnet-cni               14                  0eca9c73d9346
c33c7a91d7a85       1c7d8c51823b5                                                                               7 minutes ago        Running             kube-proxy                12                  f546b0243f43c
140c421323524       7e92ad97b3b3d                                                                               7 minutes ago        Running             nfs                       13                  c5fe2998bccc6
e794f0c432de7       4947e46903b36                                                                               7 minutes ago        Running             liveness-probe            13                  c5fe2998bccc6
918f5a33af553       a70e0499c775e                                                                               7 minutes ago        Running             csi-provisioner           14                  c5fe2998bccc6
31c18cf3ed81e       7e92ad97b3b3d                                                                               7 minutes ago        Running             nfs                       13                  58bc7ecde2404
be717815591d4       720dcdb196378                                                                               7 minutes ago        Running             node-driver-registrar     13                  58bc7ecde2404
856e47cbbc47c       4947e46903b36                                                                               7 minutes ago        Running             liveness-probe            13                  58bc7ecde2404
91f380385f517       6e38f40d628db                                                                               7 minutes ago        Running             storage-provisioner       15                  c4dd8c251e004
1e6e4f7887d02       5185b96f0becf                                                                               7 minutes ago        Running             coredns                   11                  6feb3e99d47d7
b9c1cdee5bc81       dbfceb93c69b6                                                                               7 minutes ago        Running             kube-controller-manager   13                  751c34116480d
87fc9d63e5e71       97801f8394908                                                                               7 minutes ago        Running             kube-apiserver            2                   bed6f4b85a6e7
8852760dc2f18       ca0ea1ee3cfd3                                                                               7 minutes ago        Running             kube-scheduler            12                  3ef426c6e977f
ba4805c030006       a8a176a5d5d69                                                                               7 minutes ago        Running             etcd                      13                  5460dbb9779c2
26c0bc4ccaa4d       d6e3e26021b60                                                                               7 minutes ago        Created             kindnet-cni               13                  db01d9e60bd65
5a0e7f93408a8       97801f8394908                                                                               7 minutes ago        Created             kube-apiserver            1                   fa026835d8b9c
0970992bc3cb3       a8a176a5d5d69                                                                               7 minutes ago        Created             etcd                      12                  eb1fc7afba159
bb85968a4174d       6e38f40d628db                                                                               7 minutes ago        Created             storage-provisioner       14                  bc617017f7655
80219953857a3       7e92ad97b3b3d                                                                               7 minutes ago        Exited              nfs                       12                  7f2aecf5d1919
667442ea6481b       4947e46903b36                                                                               7 minutes ago        Exited              liveness-probe            12                  7f2aecf5d1919
ecbb48f8e31bf       dbfceb93c69b6                                                                               7 minutes ago        Exited              kube-controller-manager   12                  5c4c76e6b7bf0
ba955d8a39498       ca0ea1ee3cfd3                                                                               7 minutes ago        Exited              kube-scheduler            11                  b36cf481d59ac
c4d6364a88ed0       a70e0499c775e                                                                               7 minutes ago        Exited              csi-provisioner           13                  7f2aecf5d1919
c7ef95ae4d696       7e92ad97b3b3d                                                                               7 minutes ago        Exited              nfs                       12                  d1f9d2e9f43d1
d0c5498e34168       720dcdb196378                                                                               7 minutes ago        Exited              node-driver-registrar     12                  d1f9d2e9f43d1
cb42e07a11cdb       4947e46903b36                                                                               7 minutes ago        Exited              liveness-probe            12                  d1f9d2e9f43d1
c541a71872309       1c7d8c51823b5                                                                               7 minutes ago        Exited              kube-proxy                11                  42ff7d6fcc60e
fced3810d1529       611451465cbc2                                                                               16 minutes ago       Exited              nginx                     2                   aa000f21e86f8
3b0ef8d71f963       f24d8135658e9                                                                               16 minutes ago       Exited              git-repo-syncer           3                   aa000f21e86f8
77b12dd4c32dd       5185b96f0becf                                                                               18 minutes ago       Exited              coredns                   10                  f9a346d330493
00db1bae3fe94       bitnami/cassandra@sha256:5743c45b931df6a8e39ba63ab7b629b873e4b0341416ddb5aac321e3b78b13e0   27 hours ago         Exited              cass                      0                   07df834b35df1

* 
* ==> coredns [1e6e4f7887d0] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 74073c0c68a507b50ca81d319bd4852e1242323807443dc549ab9f2fb21c8587977d5d9a7ecbfada54b5ff45c9b40d98fc730bfb6641b1b669d8fa8e6e9cea7f
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11

* 
* ==> coredns [77b12dd4c32d] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 74073c0c68a507b50ca81d319bd4852e1242323807443dc549ab9f2fb21c8587977d5d9a7ecbfada54b5ff45c9b40d98fc730bfb6641b1b669d8fa8e6e9cea7f
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               k8s-node-cassandra
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-node-cassandra
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fe869b5d4da11ba318eb84a3ac00f336411de7ba
                    minikube.k8s.io/name=k8s-node-cassandra
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_01_03T08_42_04_0700
                    minikube.k8s.io/version=v1.27.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        csi.volume.kubernetes.io/nodeid: {"nfs.csi.k8s.io":"k8s-node-cassandra"}
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 03 Jan 2023 11:42:01 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-node-cassandra
  AcquireTime:     <unset>
  RenewTime:       Wed, 04 Jan 2023 16:43:50 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 04 Jan 2023 16:41:29 +0000   Tue, 03 Jan 2023 13:48:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 04 Jan 2023 16:41:29 +0000   Tue, 03 Jan 2023 13:48:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 04 Jan 2023 16:41:29 +0000   Tue, 03 Jan 2023 13:48:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 04 Jan 2023 16:41:29 +0000   Tue, 03 Jan 2023 13:48:11 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    k8s-node-cassandra
Capacity:
  cpu:                8
  ephemeral-storage:  240785624Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16178904Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  240785624Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16178904Ki
  pods:               110
System Info:
  Machine ID:                 386775ffbc46464ea356d784cd8cd54c
  System UUID:                c3410608-b80a-4432-8679-c506e4e19558
  Boot ID:                    d8a074f3-5c34-4ba7-a4eb-47f022fc39ef
  Kernel Version:             6.0.8-arch1-1
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.18
  Kubelet Version:            v1.25.2
  Kube-Proxy Version:         v1.25.2
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (17 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     backend-6bb8dfdb48-r47vd                      500m (6%!)(MISSING)     500m (6%!)(MISSING)   300Mi (1%!)(MISSING)       300Mi (1%!)(MISSING)     7m19s
  default                     database-0                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m27s
  default                     database-1                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26h
  default                     nginx-1672746899-7cc89949b9-f9wtk             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28h
  default                     nginx-1672747041-7f6cff4985-ws9xs             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m19s
  default                     nginx-1672747475-845674b7ff-jwd2c             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7m19s
  ingress-nginx               ingress-nginx-controller-5959f988fd-7m9f7     100m (1%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         28h
  kube-system                 coredns-565d847f94-rqqr2                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     29h
  kube-system                 csi-nfs-controller-679b6d5754-lzrjk           30m (0%!)(MISSING)      0 (0%!)(MISSING)      60Mi (0%!)(MISSING)        700Mi (4%!)(MISSING)     29h
  kube-system                 csi-nfs-node-qh9kd                            30m (0%!)(MISSING)      0 (0%!)(MISSING)      60Mi (0%!)(MISSING)        500Mi (3%!)(MISSING)     29h
  kube-system                 etcd-k8s-node-cassandra                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         29h
  kube-system                 kindnet-hkj5s                                 100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      29h
  kube-system                 kube-apiserver-k8s-node-cassandra             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18m
  kube-system                 kube-controller-manager-k8s-node-cassandra    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  kube-system                 kube-proxy-2sfhm                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  kube-system                 kube-scheduler-k8s-node-cassandra             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1510m (18%!)(MISSING)  600m (7%!)(MISSING)
  memory             730Mi (4%!)(MISSING)   1720Mi (10%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 7m28s                  kube-proxy       
  Normal  Starting                 17m                    kube-proxy       
  Normal  NodeAllocatableEnforced  18m                    kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 18m                    kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  18m (x8 over 18m)      kubelet          Node k8s-node-cassandra status is now: NodeHasSufficientMemory
  Normal  NodeHasSufficientPID     18m (x7 over 18m)      kubelet          Node k8s-node-cassandra status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    18m (x8 over 18m)      kubelet          Node k8s-node-cassandra status is now: NodeHasNoDiskPressure
  Normal  RegisteredNode           17m                    node-controller  Node k8s-node-cassandra event: Registered Node k8s-node-cassandra in Controller
  Normal  Starting                 7m39s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  7m39s (x8 over 7m39s)  kubelet          Node k8s-node-cassandra status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    7m39s (x8 over 7m39s)  kubelet          Node k8s-node-cassandra status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     7m39s (x7 over 7m39s)  kubelet          Node k8s-node-cassandra status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  7m39s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           7m23s                  node-controller  Node k8s-node-cassandra event: Registered Node k8s-node-cassandra in Controller


Name:               k8s-node-cassandra-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-node-cassandra-m03
                    kubernetes.io/os=linux
Annotations:        csi.volume.kubernetes.io/nodeid: {"nfs.csi.k8s.io":"k8s-node-cassandra-m03"}
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 03 Jan 2023 13:50:44 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-node-cassandra-m03
  AcquireTime:     <unset>
  RenewTime:       Tue, 03 Jan 2023 20:34:35 +0000
Conditions:
  Type             Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----             ------    -----------------                 ------------------                ------              -------
  MemoryPressure   Unknown   Tue, 03 Jan 2023 20:34:18 +0000   Wed, 04 Jan 2023 16:26:45 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure     Unknown   Tue, 03 Jan 2023 20:34:18 +0000   Wed, 04 Jan 2023 16:26:45 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure      Unknown   Tue, 03 Jan 2023 20:34:18 +0000   Wed, 04 Jan 2023 16:26:45 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready            Unknown   Tue, 03 Jan 2023 20:34:18 +0000   Wed, 04 Jan 2023 16:26:45 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  192.168.58.4
  Hostname:    k8s-node-cassandra-m03
Capacity:
  cpu:                8
  ephemeral-storage:  240785624Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16178904Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  240785624Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16178904Ki
  pods:               110
System Info:
  Machine ID:                 386775ffbc46464ea356d784cd8cd54c
  System UUID:                d50062da-ea2c-4b05-94a8-da94dc37afe5
  Boot ID:                    2cb7364f-c347-4b21-a948-898f02f392c7
  Kernel Version:             6.0.8-arch1-1
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.18
  Kubelet Version:            v1.25.2
  Kube-Proxy Version:         v1.25.2
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (6 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     backend-6bb8dfdb48-bf6fz             500m (6%!)(MISSING)     500m (6%!)(MISSING)   300Mi (1%!)(MISSING)       300Mi (1%!)(MISSING)     17m
  default                     nginx-1672747041-7f6cff4985-snlwd    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  default                     nginx-1672747475-845674b7ff-gcccb    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17m
  kube-system                 csi-nfs-node-2n65m                   30m (0%!)(MISSING)      0 (0%!)(MISSING)      60Mi (0%!)(MISSING)        500Mi (3%!)(MISSING)     29h
  kube-system                 kindnet-zxfwt                        100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      29h
  kube-system                 kube-proxy-5nxwg                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                630m (7%!)(MISSING)   600m (7%!)(MISSING)
  memory             410Mi (2%!)(MISSING)  850Mi (5%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason          Age    From             Message
  ----    ------          ----   ----             -------
  Normal  RegisteredNode  17m    node-controller  Node k8s-node-cassandra-m03 event: Registered Node k8s-node-cassandra-m03 in Controller
  Normal  NodeNotReady    17m    node-controller  Node k8s-node-cassandra-m03 status is now: NodeNotReady
  Normal  RegisteredNode  7m23s  node-controller  Node k8s-node-cassandra-m03 event: Registered Node k8s-node-cassandra-m03 in Controller

* 
* ==> dmesg <==
* [  +0.001229] Invalid ELF header magic: != ELF
[  +0.001198] Invalid ELF header magic: != ELF
[Jan 4 16:26] kauditd_printk_skb: 967 callbacks suppressed
[  +6.670919] kauditd_printk_skb: 475 callbacks suppressed
[  +7.095249] kauditd_printk_skb: 4 callbacks suppressed
[ +10.419770] kauditd_printk_skb: 91 callbacks suppressed
[  +4.789828] Invalid ELF header magic: != ELF
[  +0.001718] Invalid ELF header magic: != ELF
[  +0.001022] Invalid ELF header magic: != ELF
[  +0.001022] Invalid ELF header magic: != ELF
[  +7.550249] kauditd_printk_skb: 155 callbacks suppressed
[ +14.683227] kauditd_printk_skb: 4 callbacks suppressed
[  +6.052358] kauditd_printk_skb: 4 callbacks suppressed
[Jan 4 16:27] kauditd_printk_skb: 8 callbacks suppressed
[ +10.397208] kauditd_printk_skb: 20 callbacks suppressed
[ +37.473950] kauditd_printk_skb: 14 callbacks suppressed
[Jan 4 16:28] kauditd_printk_skb: 6 callbacks suppressed
[Jan 4 16:29] kauditd_printk_skb: 6 callbacks suppressed
[Jan 4 16:30] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000072] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Jan 4 16:31] kauditd_printk_skb: 4 callbacks suppressed
[Jan 4 16:32] kauditd_printk_skb: 98 callbacks suppressed
[  +5.710650] kauditd_printk_skb: 20 callbacks suppressed
[  +7.307591] kauditd_printk_skb: 26 callbacks suppressed
[  +9.311264] kauditd_printk_skb: 22 callbacks suppressed
[Jan 4 16:34] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Jan 4 16:35] kauditd_printk_skb: 14 callbacks suppressed
[ +54.076294] kauditd_printk_skb: 4 callbacks suppressed
[Jan 4 16:36] kauditd_printk_skb: 606 callbacks suppressed
[  +0.119186] Invalid ELF header magic: != ELF
[  +0.001110] Invalid ELF header magic: != ELF
[  +0.001063] Invalid ELF header magic: != ELF
[  +0.000972] Invalid ELF header magic: != ELF
[  +9.281776] kauditd_printk_skb: 93 callbacks suppressed
[  +1.748909] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000435] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +3.255868] kauditd_printk_skb: 624 callbacks suppressed
[  +6.049494] kauditd_printk_skb: 451 callbacks suppressed
[  +5.002888] kauditd_printk_skb: 616 callbacks suppressed
[  +0.738112] Invalid ELF header magic: != ELF
[  +0.001214] Invalid ELF header magic: != ELF
[  +0.001324] Invalid ELF header magic: != ELF
[  +0.002109] Invalid ELF header magic: != ELF
[  +6.611088] kauditd_printk_skb: 138 callbacks suppressed
[  +5.051143] kauditd_printk_skb: 507 callbacks suppressed
[  +1.138548] Invalid ELF header magic: != ELF
[  +0.004063] Invalid ELF header magic: != ELF
[  +0.003307] Invalid ELF header magic: != ELF
[  +0.001914] Invalid ELF header magic: != ELF
[ +11.216244] kauditd_printk_skb: 122 callbacks suppressed
[  +6.919494] kauditd_printk_skb: 20 callbacks suppressed
[Jan 4 16:37] kauditd_printk_skb: 14 callbacks suppressed
[ +10.469485] kauditd_printk_skb: 20 callbacks suppressed
[  +7.553349] kauditd_printk_skb: 44 callbacks suppressed
[  +7.846665] kauditd_printk_skb: 30 callbacks suppressed
[Jan 4 16:38] kauditd_printk_skb: 2 callbacks suppressed
[ +12.036299] kauditd_printk_skb: 6 callbacks suppressed
[Jan 4 16:40] kauditd_printk_skb: 6 callbacks suppressed
[Jan 4 16:42] kauditd_printk_skb: 4 callbacks suppressed

* 
* ==> etcd [0970992bc3cb] <==
* 
* 
* ==> etcd [ba4805c03000] <==
* {"level":"info","ts":"2023-01-04T16:36:20.242Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-01-04T16:36:20.242Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2023-01-04T16:36:20.242Z","caller":"embed/etcd.go:479","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-01-04T16:36:20.243Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"]}
{"level":"info","ts":"2023-01-04T16:36:20.243Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.4","git-sha":"08407ff76","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":true,"name":"k8s-node-cassandra","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-01-04T16:36:20.246Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.09789ms"}
{"level":"info","ts":"2023-01-04T16:36:20.339Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":50005,"snapshot-size":"11 kB"}
{"level":"info","ts":"2023-01-04T16:36:20.339Z","caller":"etcdserver/server.go:521","msg":"recovered v3 backend from snapshot","backend-size-bytes":11575296,"backend-size":"12 MB","backend-size-in-use-bytes":2105344,"backend-size-in-use":"2.1 MB"}
{"level":"info","ts":"2023-01-04T16:36:20.441Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","commit-index":50332}
{"level":"info","ts":"2023-01-04T16:36:20.441Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 switched to configuration voters=(12882097698489969905)"}
{"level":"info","ts":"2023-01-04T16:36:20.441Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became follower at term 11"}
{"level":"info","ts":"2023-01-04T16:36:20.441Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b2c6679ac05f2cf1 [peers: [b2c6679ac05f2cf1], term: 11, commit: 50332, applied: 50005, lastindex: 50332, lastterm: 11]"}
{"level":"info","ts":"2023-01-04T16:36:20.441Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-01-04T16:36:20.441Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"3a56e4ca95e2355c","local-member-id":"b2c6679ac05f2cf1","recovered-remote-peer-id":"b2c6679ac05f2cf1","recovered-remote-peer-urls":["https://192.168.58.2:2380"]}
{"level":"info","ts":"2023-01-04T16:36:20.441Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-01-04T16:36:20.443Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-01-04T16:36:20.443Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":43325}
{"level":"info","ts":"2023-01-04T16:36:20.446Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":43706}
{"level":"info","ts":"2023-01-04T16:36:20.448Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-01-04T16:36:20.450Z","caller":"etcdserver/corrupt.go:46","msg":"starting initial corruption check","local-member-id":"b2c6679ac05f2cf1","timeout":"7s"}
{"level":"info","ts":"2023-01-04T16:36:20.451Z","caller":"etcdserver/corrupt.go:116","msg":"initial corruption checking passed; no corruption","local-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2023-01-04T16:36:20.451Z","caller":"etcdserver/server.go:842","msg":"starting etcd server","local-member-id":"b2c6679ac05f2cf1","local-server-version":"3.5.4","cluster-id":"3a56e4ca95e2355c","cluster-version":"3.5"}
{"level":"info","ts":"2023-01-04T16:36:20.452Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b2c6679ac05f2cf1","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-01-04T16:36:20.454Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-01-04T16:36:20.454Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2023-01-04T16:36:20.454Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.58.2:2380"}
{"level":"info","ts":"2023-01-04T16:36:20.454Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"b2c6679ac05f2cf1","initial-advertise-peer-urls":["https://192.168.58.2:2380"],"listen-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.58.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-01-04T16:36:20.454Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-01-04T16:36:21.242Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 is starting a new election at term 11"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became pre-candidate at term 11"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgPreVoteResp from b2c6679ac05f2cf1 at term 11"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became candidate at term 12"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 received MsgVoteResp from b2c6679ac05f2cf1 at term 12"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b2c6679ac05f2cf1 became leader at term 12"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b2c6679ac05f2cf1 elected leader b2c6679ac05f2cf1 at term 12"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"b2c6679ac05f2cf1","local-member-attributes":"{Name:k8s-node-cassandra ClientURLs:[https://192.168.58.2:2379]}","request-path":"/0/members/b2c6679ac05f2cf1/attributes","cluster-id":"3a56e4ca95e2355c","publish-timeout":"7s"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-01-04T16:36:21.243Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-01-04T16:36:21.244Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-01-04T16:36:21.244Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-01-04T16:36:21.245Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-01-04T16:36:21.245Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.58.2:2379"}
{"level":"info","ts":"2023-01-04T16:40:48.356Z","caller":"traceutil/trace.go:171","msg":"trace[1985576990] linearizableReadLoop","detail":"{readStateIndex:51047; appliedIndex:51047; }","duration":"113.13101ms","start":"2023-01-04T16:40:48.242Z","end":"2023-01-04T16:40:48.356Z","steps":["trace[1985576990] 'read index received'  (duration: 113.121249ms)","trace[1985576990] 'applied index is now lower than readState.Index'  (duration: 8.423µs)"],"step_count":2}
{"level":"warn","ts":"2023-01-04T16:40:48.356Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"113.254089ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-01-04T16:40:48.356Z","caller":"traceutil/trace.go:171","msg":"trace[711570624] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:44393; }","duration":"114.020482ms","start":"2023-01-04T16:40:48.242Z","end":"2023-01-04T16:40:48.356Z","steps":["trace[711570624] 'agreement among raft nodes before linearized reading'  (duration: 113.213116ms)"],"step_count":1}
{"level":"info","ts":"2023-01-04T16:43:18.788Z","caller":"traceutil/trace.go:171","msg":"trace[1143751385] linearizableReadLoop","detail":"{readStateIndex:51206; appliedIndex:51206; }","duration":"144.669178ms","start":"2023-01-04T16:43:18.643Z","end":"2023-01-04T16:43:18.788Z","steps":["trace[1143751385] 'read index received'  (duration: 144.6592ms)","trace[1143751385] 'applied index is now lower than readState.Index'  (duration: 9.087µs)"],"step_count":2}
{"level":"warn","ts":"2023-01-04T16:43:18.788Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"144.805ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-01-04T16:43:18.788Z","caller":"traceutil/trace.go:171","msg":"trace[2065660646] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:44549; }","duration":"144.890432ms","start":"2023-01-04T16:43:18.643Z","end":"2023-01-04T16:43:18.788Z","steps":["trace[2065660646] 'agreement among raft nodes before linearized reading'  (duration: 144.763945ms)"],"step_count":1}
{"level":"info","ts":"2023-01-04T16:43:54.652Z","caller":"traceutil/trace.go:171","msg":"trace[881557989] linearizableReadLoop","detail":"{readStateIndex:51245; appliedIndex:51245; }","duration":"108.238011ms","start":"2023-01-04T16:43:54.544Z","end":"2023-01-04T16:43:54.652Z","steps":["trace[881557989] 'read index received'  (duration: 108.228382ms)","trace[881557989] 'applied index is now lower than readState.Index'  (duration: 8.358µs)"],"step_count":2}
{"level":"warn","ts":"2023-01-04T16:43:54.653Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"109.29409ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-01-04T16:43:54.653Z","caller":"traceutil/trace.go:171","msg":"trace[444402104] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:44588; }","duration":"109.3318ms","start":"2023-01-04T16:43:54.544Z","end":"2023-01-04T16:43:54.653Z","steps":["trace[444402104] 'agreement among raft nodes before linearized reading'  (duration: 108.339576ms)"],"step_count":1}
{"level":"info","ts":"2023-01-04T16:43:55.975Z","caller":"traceutil/trace.go:171","msg":"trace[1089684905] linearizableReadLoop","detail":"{readStateIndex:51247; appliedIndex:51247; }","duration":"192.108635ms","start":"2023-01-04T16:43:55.783Z","end":"2023-01-04T16:43:55.975Z","steps":["trace[1089684905] 'read index received'  (duration: 192.08035ms)","trace[1089684905] 'applied index is now lower than readState.Index'  (duration: 26.539µs)"],"step_count":2}
{"level":"warn","ts":"2023-01-04T16:43:56.098Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"254.936376ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2023-01-04T16:43:56.098Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"315.065979ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/backend-6bb8dfdb48-r47vd.173728807b4e51d7\" ","response":"range_response_count:1 size:672"}
{"level":"info","ts":"2023-01-04T16:43:56.098Z","caller":"traceutil/trace.go:171","msg":"trace[1039101377] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:44590; }","duration":"255.025528ms","start":"2023-01-04T16:43:55.843Z","end":"2023-01-04T16:43:56.098Z","steps":["trace[1039101377] 'agreement among raft nodes before linearized reading'  (duration: 132.168405ms)","trace[1039101377] 'range keys from in-memory index tree'  (duration: 122.739082ms)"],"step_count":2}
{"level":"info","ts":"2023-01-04T16:43:56.098Z","caller":"traceutil/trace.go:171","msg":"trace[1425189726] range","detail":"{range_begin:/registry/events/default/backend-6bb8dfdb48-r47vd.173728807b4e51d7; range_end:; response_count:1; response_revision:44590; }","duration":"315.187076ms","start":"2023-01-04T16:43:55.783Z","end":"2023-01-04T16:43:56.098Z","steps":["trace[1425189726] 'agreement among raft nodes before linearized reading'  (duration: 192.221621ms)","trace[1425189726] 'range keys from in-memory index tree'  (duration: 122.75362ms)"],"step_count":2}
{"level":"warn","ts":"2023-01-04T16:43:56.099Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-01-04T16:43:55.783Z","time spent":"315.266739ms","remote":"127.0.0.1:41758","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":1,"response size":695,"request content":"key:\"/registry/events/default/backend-6bb8dfdb48-r47vd.173728807b4e51d7\" "}
{"level":"info","ts":"2023-01-04T16:43:56.899Z","caller":"traceutil/trace.go:171","msg":"trace[1784443578] linearizableReadLoop","detail":"{readStateIndex:51249; appliedIndex:51249; }","duration":"156.275288ms","start":"2023-01-04T16:43:56.743Z","end":"2023-01-04T16:43:56.899Z","steps":["trace[1784443578] 'read index received'  (duration: 156.267444ms)","trace[1784443578] 'applied index is now lower than readState.Index'  (duration: 6.907µs)"],"step_count":2}
{"level":"warn","ts":"2023-01-04T16:43:56.954Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"211.41145ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-01-04T16:43:56.954Z","caller":"traceutil/trace.go:171","msg":"trace[1219659236] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:44592; }","duration":"211.505795ms","start":"2023-01-04T16:43:56.743Z","end":"2023-01-04T16:43:56.954Z","steps":["trace[1219659236] 'agreement among raft nodes before linearized reading'  (duration: 156.389085ms)","trace[1219659236] 'range keys from in-memory index tree'  (duration: 54.99769ms)"],"step_count":2}

* 
* ==> kernel <==
*  16:43:57 up  5:36,  0 users,  load average: 2.80, 2.09, 2.16
Linux k8s-node-cassandra 6.0.8-arch1-1 #1 SMP PREEMPT_DYNAMIC Thu, 10 Nov 2022 21:14:24 +0000 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [5a0e7f93408a] <==
* 
* 
* ==> kube-apiserver [87fc9d63e5e7] <==
* W0104 16:43:43.215604       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:43.215615       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:44.118770       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:44.118783       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:44.217833       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:44.217848       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:45.121235       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:45.121262       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:45.220631       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:45.220655       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:46.123008       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:46.123021       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:46.222972       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:46.222985       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
E0104 16:43:46.384221       1 status.go:71] apiserver received an error that is not an metav1.Status: 3: illegal base64 data at input byte 3
W0104 16:43:47.125053       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:47.125073       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:47.225342       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:47.225362       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:48.127113       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:48.127137       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:48.227619       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:48.227641       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:49.128867       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:49.128890       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:49.230369       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:49.230391       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:50.130733       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:50.130745       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:50.233232       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:50.233264       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:51.132870       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:51.132899       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:51.235781       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:51.235808       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:52.135424       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:52.135452       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:52.238767       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:52.238788       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:53.137711       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:53.137732       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:53.241301       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:53.241325       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:54.140094       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:54.140106       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:54.244415       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:54.244466       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:55.142177       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:55.142205       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:55.246345       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:55.246356       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
E0104 16:43:55.760574       1 status.go:71] apiserver received an error that is not an metav1.Status: 3: illegal base64 data at input byte 3
W0104 16:43:56.144822       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:56.144851       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:56.247330       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:56.247342       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:57.146066       1 reflector.go:424] storage/cacher.go:/configmaps: failed to list *core.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:57.146080       1 cacher.go:440] cacher (*core.ConfigMap): unexpected ListAndWatch error: failed to list *core.ConfigMap: illegal base64 data at input byte 3; reinitializing...
W0104 16:43:57.248560       1 reflector.go:424] storage/cacher.go:/secrets: failed to list *core.Secret: illegal base64 data at input byte 3
E0104 16:43:57.248573       1 cacher.go:440] cacher (*core.Secret): unexpected ListAndWatch error: failed to list *core.Secret: illegal base64 data at input byte 3; reinitializing...

* 
* ==> kube-controller-manager [b9c1cdee5bc8] <==
* I0104 16:40:05.325253       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W0104 16:40:12.459270       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:40:12.459301       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
W0104 16:40:21.505744       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Secret: illegal base64 data at input byte 3
E0104 16:40:21.505784       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: failed to list *v1.Secret: illegal base64 data at input byte 3
I0104 16:40:34.627842       1 shared_informer.go:255] Waiting for caches to sync for resource quota
E0104 16:40:35.326531       1 shared_informer.go:258] unable to sync caches for garbage collector
E0104 16:40:35.326559       1 garbagecollector.go:250] timed out waiting for dependency graph builder sync during GC sync (attempt 8)
I0104 16:40:35.422658       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W0104 16:40:52.467544       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:40:52.467575       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:41:04.628055       1 shared_informer.go:258] unable to sync caches for resource quota
E0104 16:41:04.628068       1 resource_quota_controller.go:456] timed out waiting for quota monitor sync
E0104 16:41:05.422824       1 shared_informer.go:258] unable to sync caches for garbage collector
E0104 16:41:05.422848       1 garbagecollector.go:250] timed out waiting for dependency graph builder sync during GC sync (attempt 9)
I0104 16:41:05.522811       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W0104 16:41:20.079792       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Secret: illegal base64 data at input byte 3
E0104 16:41:20.079811       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: failed to list *v1.Secret: illegal base64 data at input byte 3
I0104 16:41:34.632895       1 shared_informer.go:255] Waiting for caches to sync for resource quota
I0104 16:41:34.887092       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="default/nginx-1672747475-845674b7ff-gcccb"
I0104 16:41:34.887200       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="default/backend-6bb8dfdb48-bf6fz"
I0104 16:41:34.887308       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="default/nginx-1672747041-7f6cff4985-snlwd"
I0104 16:41:34.887375       1 event.go:294] "Event occurred" object="default/nginx-1672747475-845674b7ff-gcccb" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Marking for deletion Pod default/nginx-1672747475-845674b7ff-gcccb"
I0104 16:41:34.887419       1 event.go:294] "Event occurred" object="default/backend-6bb8dfdb48-bf6fz" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Marking for deletion Pod default/backend-6bb8dfdb48-bf6fz"
I0104 16:41:34.887488       1 event.go:294] "Event occurred" object="default/nginx-1672747041-7f6cff4985-snlwd" fieldPath="" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Marking for deletion Pod default/nginx-1672747041-7f6cff4985-snlwd"
E0104 16:41:35.523187       1 shared_informer.go:258] unable to sync caches for garbage collector
E0104 16:41:35.523223       1 garbagecollector.go:250] timed out waiting for dependency graph builder sync during GC sync (attempt 10)
I0104 16:41:35.620642       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W0104 16:41:41.749757       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:41:41.749774       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:42:04.633505       1 shared_informer.go:258] unable to sync caches for resource quota
E0104 16:42:04.633519       1 resource_quota_controller.go:456] timed out waiting for quota monitor sync
E0104 16:42:05.620748       1 shared_informer.go:258] unable to sync caches for garbage collector
E0104 16:42:05.620774       1 garbagecollector.go:250] timed out waiting for dependency graph builder sync during GC sync (attempt 11)
I0104 16:42:05.725097       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W0104 16:42:16.743214       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:42:16.743231       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
W0104 16:42:19.237351       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Secret: illegal base64 data at input byte 3
E0104 16:42:19.237393       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: failed to list *v1.Secret: illegal base64 data at input byte 3
I0104 16:42:34.638120       1 shared_informer.go:255] Waiting for caches to sync for resource quota
E0104 16:42:35.725248       1 shared_informer.go:258] unable to sync caches for garbage collector
E0104 16:42:35.725272       1 garbagecollector.go:250] timed out waiting for dependency graph builder sync during GC sync (attempt 12)
I0104 16:42:35.820306       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W0104 16:43:04.464929       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Secret: illegal base64 data at input byte 3
E0104 16:43:04.464945       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: failed to list *v1.Secret: illegal base64 data at input byte 3
E0104 16:43:04.638297       1 shared_informer.go:258] unable to sync caches for resource quota
E0104 16:43:04.638314       1 resource_quota_controller.go:456] timed out waiting for quota monitor sync
W0104 16:43:05.182160       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:05.182194       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:05.820572       1 shared_informer.go:258] unable to sync caches for garbage collector
E0104 16:43:05.820607       1 garbagecollector.go:250] timed out waiting for dependency graph builder sync during GC sync (attempt 13)
I0104 16:43:05.928378       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0104 16:43:34.643406       1 shared_informer.go:255] Waiting for caches to sync for resource quota
E0104 16:43:35.928976       1 shared_informer.go:258] unable to sync caches for garbage collector
E0104 16:43:35.929009       1 garbagecollector.go:250] timed out waiting for dependency graph builder sync during GC sync (attempt 14)
I0104 16:43:36.023829       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W0104 16:43:46.384500       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
E0104 16:43:46.384518       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: illegal base64 data at input byte 3
W0104 16:43:55.761001       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Secret: illegal base64 data at input byte 3
E0104 16:43:55.761034       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Secret: failed to list *v1.Secret: illegal base64 data at input byte 3

* 
* ==> kube-controller-manager [ecbb48f8e31b] <==
* I0104 16:36:13.431713       1 serving.go:348] Generated self-signed cert in-memory
I0104 16:36:13.712096       1 controllermanager.go:178] Version: v1.25.2
I0104 16:36:13.712124       1 controllermanager.go:180] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0104 16:36:13.713355       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0104 16:36:13.713595       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0104 16:36:13.714069       1 secure_serving.go:210] Serving securely on 127.0.0.1:10257
I0104 16:36:13.714976       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"

* 
* ==> kube-proxy [c33c7a91d7a8] <==
* I0104 16:36:28.374718       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I0104 16:36:28.375835       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I0104 16:36:28.377286       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I0104 16:36:28.379469       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I0104 16:36:28.388554       1 node.go:163] Successfully retrieved node IP: 192.168.58.2
I0104 16:36:28.388576       1 server_others.go:138] "Detected node IP" address="192.168.58.2"
I0104 16:36:28.388591       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0104 16:36:28.425228       1 server_others.go:206] "Using iptables Proxier"
I0104 16:36:28.425252       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0104 16:36:28.425267       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0104 16:36:28.425327       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0104 16:36:28.425354       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0104 16:36:28.425464       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0104 16:36:28.425666       1 server.go:661] "Version info" version="v1.25.2"
I0104 16:36:28.425675       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0104 16:36:28.426175       1 config.go:444] "Starting node config controller"
I0104 16:36:28.426181       1 shared_informer.go:255] Waiting for caches to sync for node config
I0104 16:36:28.427045       1 config.go:226] "Starting endpoint slice config controller"
I0104 16:36:28.427107       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0104 16:36:28.427043       1 config.go:317] "Starting service config controller"
I0104 16:36:28.427152       1 shared_informer.go:255] Waiting for caches to sync for service config
I0104 16:36:28.527031       1 shared_informer.go:262] Caches are synced for node config
I0104 16:36:28.528052       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0104 16:36:28.528056       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-proxy [c541a7187230] <==
* I0104 16:36:02.294300       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I0104 16:36:02.295444       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I0104 16:36:02.296451       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I0104 16:36:02.297446       1 proxier.go:666] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"

* 
* ==> kube-scheduler [8852760dc2f1] <==
* I0104 16:36:20.507915       1 serving.go:348] Generated self-signed cert in-memory
W0104 16:36:22.520875       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0104 16:36:22.520947       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0104 16:36:22.520966       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0104 16:36:22.520990       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0104 16:36:22.550483       1 server.go:148] "Starting Kubernetes Scheduler" version="v1.25.2"
I0104 16:36:22.550552       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0104 16:36:22.552539       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0104 16:36:22.552600       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0104 16:36:22.552999       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0104 16:36:22.553046       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0104 16:36:22.654619       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [ba955d8a3949] <==
* W0104 16:36:13.867661       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.867685       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.867694       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:13.867733       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.867754       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:13.867819       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:13.867819       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.867840       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.867844       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:13.867902       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.867920       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:13.867983       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:13.867979       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.868004       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:13.868007       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.734327       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.734359       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.741628       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.741657       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.753069       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.753090       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.828475       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.828545       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.836036       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.836082       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.855683       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.855719       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.889843       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.889892       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:14.929623       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:14.929660       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:15.018740       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:15.018782       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:15.019098       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:15.019134       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:15.065116       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:15.065163       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:15.209656       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:15.209701       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:15.294318       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:15.294347       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:15.324418       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:15.324509       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:15.463126       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:15.463184       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:16.756654       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:16.756694       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:17.001262       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:17.001306       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:17.115420       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:17.115525       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:17.286526       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:17.286600       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0104 16:36:17.306934       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0104 16:36:17.306972       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
I0104 16:36:17.494985       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I0104 16:36:17.495076       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0104 16:36:17.495147       1 shared_informer.go:258] unable to sync caches for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0104 16:36:17.495157       1 configmap_cafile_content.go:210] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0104 16:36:17.495675       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* -- Logs begin at Wed 2023-01-04 16:25:34 UTC, end at Wed 2023-01-04 16:43:57 UTC. --
Jan 04 16:40:28 k8s-node-cassandra kubelet[24935]: E0104 16:40:28.002335   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:40:38 k8s-node-cassandra kubelet[24935]: E0104 16:40:38.410905   24935 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-20810039-66f9-42d2-89ce-e174aa1676ff# podName: nodeName:}" failed. No retries permitted until 2023-01-04 16:42:40.410887232 +0000 UTC m=+381.851262498 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "pvc-20810039-66f9-42d2-89ce-e174aa1676ff" (UniqueName: "kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-20810039-66f9-42d2-89ce-e174aa1676ff#") pod "database-1" (UID: "ea472dbe-27f4-4777-b9b1-a05af1c5912a") : rpc error: code = Internal desc = mount failed: exit status 32
Jan 04 16:40:38 k8s-node-cassandra kubelet[24935]: Mounting command: mount
Jan 04 16:40:38 k8s-node-cassandra kubelet[24935]: Mounting arguments: -t nfs -o hard,nfsvers=4.1 nfs-server:/exports/pvc-20810039-66f9-42d2-89ce-e174aa1676ff /var/lib/kubelet/pods/ea472dbe-27f4-4777-b9b1-a05af1c5912a/volumes/kubernetes.io~csi/pvc-20810039-66f9-42d2-89ce-e174aa1676ff/mount
Jan 04 16:40:38 k8s-node-cassandra kubelet[24935]: Output: mount.nfs: Failed to resolve server nfs-server: Name or service not known
Jan 04 16:40:39 k8s-node-cassandra kubelet[24935]: I0104 16:40:39.767190   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:40:39 k8s-node-cassandra kubelet[24935]: E0104 16:40:39.767430   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:40:43 k8s-node-cassandra kubelet[24935]: E0104 16:40:43.767869   24935 kubelet.go:1731] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[nfs], unattached volumes=[kube-api-access-f5cl4 cql-scripts nfs]: timed out waiting for the condition" pod="default/database-1"
Jan 04 16:40:43 k8s-node-cassandra kubelet[24935]: E0104 16:40:43.767929   24935 pod_workers.go:965] "Error syncing pod, skipping" err="unmounted volumes=[nfs], unattached volumes=[kube-api-access-f5cl4 cql-scripts nfs]: timed out waiting for the condition" pod="default/database-1" podUID=ea472dbe-27f4-4777-b9b1-a05af1c5912a
Jan 04 16:40:49 k8s-node-cassandra kubelet[24935]: E0104 16:40:49.772675   24935 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="brunopec/backend:v1.0.9"
Jan 04 16:40:49 k8s-node-cassandra kubelet[24935]: E0104 16:40:49.772716   24935 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="brunopec/backend:v1.0.9"
Jan 04 16:40:49 k8s-node-cassandra kubelet[24935]: E0104 16:40:49.772817   24935 kuberuntime_manager.go:862] container &Container{Name:backend,Image:brunopec/backend:v1.0.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:db-auth,ReadOnly:false,MountPath:/k8s/secrets,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-jcvn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod backend-6bb8dfdb48-r47vd_default(dd2af4b3-c34e-49de-8997-360e50edf78c): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Jan 04 16:40:49 k8s-node-cassandra kubelet[24935]: E0104 16:40:49.772850   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/backend-6bb8dfdb48-r47vd" podUID=dd2af4b3-c34e-49de-8997-360e50edf78c
Jan 04 16:40:53 k8s-node-cassandra kubelet[24935]: I0104 16:40:53.766583   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:40:53 k8s-node-cassandra kubelet[24935]: E0104 16:40:53.766831   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:41:02 k8s-node-cassandra kubelet[24935]: E0104 16:41:02.767875   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"brunopec/backend:v1.0.9\\\"\"" pod="default/backend-6bb8dfdb48-r47vd" podUID=dd2af4b3-c34e-49de-8997-360e50edf78c
Jan 04 16:41:06 k8s-node-cassandra kubelet[24935]: I0104 16:41:06.767288   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:41:06 k8s-node-cassandra kubelet[24935]: E0104 16:41:06.767515   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:41:17 k8s-node-cassandra kubelet[24935]: I0104 16:41:17.766799   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:41:17 k8s-node-cassandra kubelet[24935]: E0104 16:41:17.767083   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:41:32 k8s-node-cassandra kubelet[24935]: I0104 16:41:32.766982   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:41:32 k8s-node-cassandra kubelet[24935]: E0104 16:41:32.767363   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:41:44 k8s-node-cassandra kubelet[24935]: I0104 16:41:44.766445   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:41:44 k8s-node-cassandra kubelet[24935]: E0104 16:41:44.766678   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:41:47 k8s-node-cassandra kubelet[24935]: E0104 16:41:47.269888   24935 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-8e637bea-be12-4cf7-b27c-aed89e63410e# podName: nodeName:}" failed. No retries permitted until 2023-01-04 16:43:49.269852469 +0000 UTC m=+450.710227766 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "pvc-8e637bea-be12-4cf7-b27c-aed89e63410e" (UniqueName: "kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-8e637bea-be12-4cf7-b27c-aed89e63410e#") pod "database-0" (UID: "d9ca983c-0a2d-4bde-8fda-dd0d608032cd") : rpc error: code = Internal desc = mount failed: exit status 32
Jan 04 16:41:47 k8s-node-cassandra kubelet[24935]: Mounting command: mount
Jan 04 16:41:47 k8s-node-cassandra kubelet[24935]: Mounting arguments: -t nfs -o hard,nfsvers=4.1 nfs-server:/exports/pvc-8e637bea-be12-4cf7-b27c-aed89e63410e /var/lib/kubelet/pods/d9ca983c-0a2d-4bde-8fda-dd0d608032cd/volumes/kubernetes.io~csi/pvc-8e637bea-be12-4cf7-b27c-aed89e63410e/mount
Jan 04 16:41:47 k8s-node-cassandra kubelet[24935]: Output: mount.nfs: Failed to resolve server nfs-server: Name or service not known
Jan 04 16:41:47 k8s-node-cassandra kubelet[24935]: E0104 16:41:47.768181   24935 kubelet.go:1731] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[nfs], unattached volumes=[nfs kube-api-access-pbr5j cql-scripts]: timed out waiting for the condition" pod="default/database-0"
Jan 04 16:41:47 k8s-node-cassandra kubelet[24935]: E0104 16:41:47.768205   24935 pod_workers.go:965] "Error syncing pod, skipping" err="unmounted volumes=[nfs], unattached volumes=[nfs kube-api-access-pbr5j cql-scripts]: timed out waiting for the condition" pod="default/database-0" podUID=d9ca983c-0a2d-4bde-8fda-dd0d608032cd
Jan 04 16:41:58 k8s-node-cassandra kubelet[24935]: I0104 16:41:58.766796   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:42:40 k8s-node-cassandra kubelet[24935]: E0104 16:42:40.537989   24935 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-20810039-66f9-42d2-89ce-e174aa1676ff# podName: nodeName:}" failed. No retries permitted until 2023-01-04 16:44:42.537963629 +0000 UTC m=+503.978338896 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "pvc-20810039-66f9-42d2-89ce-e174aa1676ff" (UniqueName: "kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-20810039-66f9-42d2-89ce-e174aa1676ff#") pod "database-1" (UID: "ea472dbe-27f4-4777-b9b1-a05af1c5912a") : rpc error: code = Internal desc = mount failed: exit status 32
Jan 04 16:42:40 k8s-node-cassandra kubelet[24935]: Mounting command: mount
Jan 04 16:42:40 k8s-node-cassandra kubelet[24935]: Mounting arguments: -t nfs -o hard,nfsvers=4.1 nfs-server:/exports/pvc-20810039-66f9-42d2-89ce-e174aa1676ff /var/lib/kubelet/pods/ea472dbe-27f4-4777-b9b1-a05af1c5912a/volumes/kubernetes.io~csi/pvc-20810039-66f9-42d2-89ce-e174aa1676ff/mount
Jan 04 16:42:40 k8s-node-cassandra kubelet[24935]: Output: mount.nfs: Failed to resolve server nfs-server: Name or service not known
Jan 04 16:42:56 k8s-node-cassandra kubelet[24935]: E0104 16:42:56.901190   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:42:57 k8s-node-cassandra kubelet[24935]: I0104 16:42:57.769413   24935 scope.go:115] "RemoveContainer" containerID="1fcf6fbfd689fb6d185e4a34fcf5a074e555fdb8d96d2f8ce304f5f807ee17b0"
Jan 04 16:42:57 k8s-node-cassandra kubelet[24935]: I0104 16:42:57.769628   24935 scope.go:115] "RemoveContainer" containerID="1a1ac21e6f83d8632f7f87cfa08ae61258e4ca302eb361792bc708ab166cf451"
Jan 04 16:42:57 k8s-node-cassandra kubelet[24935]: E0104 16:42:57.769868   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:42:59 k8s-node-cassandra kubelet[24935]: E0104 16:42:59.767301   24935 kubelet.go:1731] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[nfs], unattached volumes=[cql-scripts nfs kube-api-access-f5cl4]: timed out waiting for the condition" pod="default/database-1"
Jan 04 16:42:59 k8s-node-cassandra kubelet[24935]: E0104 16:42:59.767326   24935 pod_workers.go:965] "Error syncing pod, skipping" err="unmounted volumes=[nfs], unattached volumes=[cql-scripts nfs kube-api-access-f5cl4]: timed out waiting for the condition" pod="default/database-1" podUID=ea472dbe-27f4-4777-b9b1-a05af1c5912a
Jan 04 16:43:10 k8s-node-cassandra kubelet[24935]: I0104 16:43:10.766493   24935 scope.go:115] "RemoveContainer" containerID="1a1ac21e6f83d8632f7f87cfa08ae61258e4ca302eb361792bc708ab166cf451"
Jan 04 16:43:10 k8s-node-cassandra kubelet[24935]: E0104 16:43:10.766777   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:43:16 k8s-node-cassandra kubelet[24935]: E0104 16:43:16.767969   24935 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="brunopec/backend:v1.0.9"
Jan 04 16:43:16 k8s-node-cassandra kubelet[24935]: E0104 16:43:16.767998   24935 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="brunopec/backend:v1.0.9"
Jan 04 16:43:16 k8s-node-cassandra kubelet[24935]: E0104 16:43:16.768076   24935 kuberuntime_manager.go:862] container &Container{Name:backend,Image:brunopec/backend:v1.0.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:db-auth,ReadOnly:false,MountPath:/k8s/secrets,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-jcvn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod backend-6bb8dfdb48-r47vd_default(dd2af4b3-c34e-49de-8997-360e50edf78c): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Jan 04 16:43:16 k8s-node-cassandra kubelet[24935]: E0104 16:43:16.768100   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="default/backend-6bb8dfdb48-r47vd" podUID=dd2af4b3-c34e-49de-8997-360e50edf78c
Jan 04 16:43:25 k8s-node-cassandra kubelet[24935]: I0104 16:43:25.766960   24935 scope.go:115] "RemoveContainer" containerID="1a1ac21e6f83d8632f7f87cfa08ae61258e4ca302eb361792bc708ab166cf451"
Jan 04 16:43:25 k8s-node-cassandra kubelet[24935]: E0104 16:43:25.767298   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:43:29 k8s-node-cassandra kubelet[24935]: E0104 16:43:29.767350   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"brunopec/backend:v1.0.9\\\"\"" pod="default/backend-6bb8dfdb48-r47vd" podUID=dd2af4b3-c34e-49de-8997-360e50edf78c
Jan 04 16:43:37 k8s-node-cassandra kubelet[24935]: I0104 16:43:37.766590   24935 scope.go:115] "RemoveContainer" containerID="1a1ac21e6f83d8632f7f87cfa08ae61258e4ca302eb361792bc708ab166cf451"
Jan 04 16:43:37 k8s-node-cassandra kubelet[24935]: E0104 16:43:37.766872   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:43:40 k8s-node-cassandra kubelet[24935]: E0104 16:43:40.767693   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"brunopec/backend:v1.0.9\\\"\"" pod="default/backend-6bb8dfdb48-r47vd" podUID=dd2af4b3-c34e-49de-8997-360e50edf78c
Jan 04 16:43:49 k8s-node-cassandra kubelet[24935]: E0104 16:43:49.374906   24935 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-8e637bea-be12-4cf7-b27c-aed89e63410e# podName: nodeName:}" failed. No retries permitted until 2023-01-04 16:45:51.374884997 +0000 UTC m=+572.815260262 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "pvc-8e637bea-be12-4cf7-b27c-aed89e63410e" (UniqueName: "kubernetes.io/csi/nfs.csi.k8s.io^nfs-server#exports#pvc-8e637bea-be12-4cf7-b27c-aed89e63410e#") pod "database-0" (UID: "d9ca983c-0a2d-4bde-8fda-dd0d608032cd") : rpc error: code = Internal desc = mount failed: exit status 32
Jan 04 16:43:49 k8s-node-cassandra kubelet[24935]: Mounting command: mount
Jan 04 16:43:49 k8s-node-cassandra kubelet[24935]: Mounting arguments: -t nfs -o hard,nfsvers=4.1 nfs-server:/exports/pvc-8e637bea-be12-4cf7-b27c-aed89e63410e /var/lib/kubelet/pods/d9ca983c-0a2d-4bde-8fda-dd0d608032cd/volumes/kubernetes.io~csi/pvc-8e637bea-be12-4cf7-b27c-aed89e63410e/mount
Jan 04 16:43:49 k8s-node-cassandra kubelet[24935]: Output: mount.nfs: Failed to resolve server nfs-server: Name or service not known
Jan 04 16:43:51 k8s-node-cassandra kubelet[24935]: I0104 16:43:51.766603   24935 scope.go:115] "RemoveContainer" containerID="1a1ac21e6f83d8632f7f87cfa08ae61258e4ca302eb361792bc708ab166cf451"
Jan 04 16:43:51 k8s-node-cassandra kubelet[24935]: E0104 16:43:51.766824   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=controller pod=ingress-nginx-controller-5959f988fd-7m9f7_ingress-nginx(9419c7d1-9e7d-47ff-826e-76c2ef7c45b1)\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-7m9f7" podUID=9419c7d1-9e7d-47ff-826e-76c2ef7c45b1
Jan 04 16:43:55 k8s-node-cassandra kubelet[24935]: E0104 16:43:55.778716   24935 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ImagePullBackOff: \"Back-off pulling image \\\"brunopec/backend:v1.0.9\\\"\"" pod="default/backend-6bb8dfdb48-r47vd" podUID=dd2af4b3-c34e-49de-8997-360e50edf78c

* 
* ==> storage-provisioner [91f380385f51] <==
* I0104 16:36:26.121416       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0104 16:36:26.130483       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0104 16:36:26.130521       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0104 16:36:43.525691       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0104 16:36:43.525820       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_k8s-node-cassandra_7afe9600-8fb6-4eae-bb0c-26040451f67a!
I0104 16:36:43.525827       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"6aabb250-b2c6-46b9-8ab1-977f73e1063b", APIVersion:"v1", ResourceVersion:"44012", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' k8s-node-cassandra_7afe9600-8fb6-4eae-bb0c-26040451f67a became leader
I0104 16:36:43.626031       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_k8s-node-cassandra_7afe9600-8fb6-4eae-bb0c-26040451f67a!

* 
* ==> storage-provisioner [bb85968a4174] <==
* 
